{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853a9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MNIST.csv')\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape\n",
    "\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:n]\n",
    "X_test = X_test / 255\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255\n",
    "\n",
    "_, m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025dbe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 41000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9031288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size, learning_rate, data_num, bias = 1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.m = data_num\n",
    "\n",
    "        # self.W1 = np.random.rand(hidden_1_size, input_size) - 0.5\n",
    "        # self.b1 = np.random.rand(hidden_1_size, bias) - 0.5\n",
    "\n",
    "        # self.W2 = np.random.rand(hidden_2_size, hidden_1_size) - 0.5\n",
    "        # self.b2 = np.random.rand(hidden_2_size, bias) - 0.5\n",
    "\n",
    "        # self.W3 = np.random.rand(output_size, hidden_2_size) - 0.5\n",
    "        # self.b3 = np.random.rand(output_size, bias) - 0.5\n",
    "\n",
    "        # Xavier initialization for weights\n",
    "        self.W1 = np.random.randn(hidden_1_size, input_size) * np.sqrt(1.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_1_size, bias))\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_2_size, hidden_1_size) * np.sqrt(1.0 / hidden_1_size)\n",
    "        self.b2 = np.zeros((hidden_2_size, bias))\n",
    "\n",
    "        self.W3 = np.random.randn(output_size, hidden_2_size) * np.sqrt(1.0 / hidden_2_size)\n",
    "        self.b3 = np.zeros((output_size, bias))\n",
    "\n",
    "    def set_learning_rate(self, new_LR):\n",
    "        self.learning_rate = new_LR\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        self.Z1 = self.W1.dot(X) + self.b1\n",
    "        self.A1 = ReLU(self.Z1)\n",
    "\n",
    "        self.Z2 = self.W2.dot(self.A1) + self.b2\n",
    "        self.A2 = ReLU(self.Z2)\n",
    "\n",
    "        self.Z3 = self.W3.dot(self.A2) + self.b3\n",
    "        self.A3 = softmax(self.Z3)\n",
    "\n",
    "        return self.A3, self.Z3\n",
    "\n",
    "    def backward_prop(self, X, y):\n",
    "        one_hot_Y = one_hot(y)\n",
    "\n",
    "        self.dZ3 = self.A3 - one_hot_Y\n",
    "        self.dW3 = 1 / self.m * self.dZ3.dot(self.A2.T)\n",
    "        self.db3 = 1 / self.m * np.sum(self.dZ3, axis = 1, keepdims = True)\n",
    "\n",
    "        self.dZ2 = self.W3.T.dot(self.dZ3) * ReLU_prime(self.Z2)\n",
    "        self.dW2 = 1 / self.m * self.dZ2.dot(self.A1.T)\n",
    "        self.db2 = 1 / self.m * np.sum(self.dZ2, axis = 1, keepdims = True)\n",
    "\n",
    "        self.dZ1 = self.W2.T.dot(self.dZ2) * ReLU_prime(self.Z1)\n",
    "        self.dW1 = 1 / self.m * self.dZ1.dot(X.T)\n",
    "        self.db1 = 1 / self.m * np.sum(self.dZ1, axis = 1, keepdims = True)\n",
    "\n",
    "        #return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        self.W1 = self.W1 - self.dW1 * self.learning_rate\n",
    "        self.b1 = self.b1 - self.db1 * self.learning_rate\n",
    "\n",
    "        self.W2 = self.W2 - self.dW2 * self.learning_rate\n",
    "        self.b2 = self.b2 - self.db2 * self.learning_rate\n",
    "\n",
    "        self.W3 = self.W3 - self.dW3 * self.learning_rate\n",
    "        self.b3 = self.b3 - self.db3 * self.learning_rate\n",
    "\n",
    "\n",
    "    def train_GD(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "            self.gradient_descent()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                predictions = np.argmax(self.A3, 0)\n",
    "                print(\"Epoch\", epoch, \":\", round((np.sum(predictions == Y_train) / Y_train.size) * 100, 10), \"%\")\n",
    "        print(\"Training complete\")           \n",
    "\n",
    "            \n",
    "\n",
    "# def get_predictions(A3):\n",
    "#     return np.argmax(A3, 0)\n",
    "\n",
    "# def get_accuracy(predictions, Y):\n",
    "#     return round((np.sum(predictions == Y) / Y.size) * 100, 2)\n",
    "        \n",
    "\n",
    "class PReLU:\n",
    "    def __init__(self, alpha_init = 0.01):\n",
    "        self.alpha = alpha_init\n",
    "\n",
    "    def forward(self, Z):\n",
    "        return np.maximum(self.alpha * Z, Z)\n",
    "\n",
    "    def prime(self, Z):\n",
    "        return np.where(Z > 0, 1, self.alpha)\n",
    "\n",
    "def update_alpha(alpha, dZ):\n",
    "    learning_rate = 0.01\n",
    "    alpha -= learning_rate * np.mean(np.where(dZ < 0, dZ * alpha, 0))\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_prime(Z):\n",
    "    return Z > 0\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(np.clip(-Z, -4, 4)))\n",
    "    return A\n",
    "\n",
    "def sigmoid_prime(Z):\n",
    "    A = (sigmoid(Z) * (1 - sigmoid(Z)))\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b208f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(784, 64, 32, 10, 0.05, 41000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3950c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 99.94 %\n",
      "Epoch 10 : 99.94 %\n",
      "Epoch 20 : 99.94 %\n",
      "Epoch 30 : 99.94 %\n",
      "Epoch 40 : 99.94 %\n",
      "Epoch 50 : 99.94 %\n",
      "Epoch 60 : 99.94 %\n",
      "Epoch 70 : 99.94 %\n",
      "Epoch 80 : 99.94 %\n",
      "Epoch 90 : 99.94 %\n",
      "Epoch 100 : 99.94 %\n",
      "Epoch 110 : 99.94 %\n",
      "Epoch 120 : 99.94 %\n",
      "Epoch 130 : 99.94 %\n",
      "Epoch 140 : 99.94 %\n",
      "Epoch 150 : 99.94 %\n",
      "Epoch 160 : 99.94 %\n",
      "Epoch 170 : 99.94 %\n",
      "Epoch 180 : 99.94 %\n",
      "Epoch 190 : 99.94 %\n",
      "Epoch 200 : 99.94 %\n",
      "Epoch 210 : 99.94 %\n",
      "Epoch 220 : 99.94 %\n",
      "Epoch 230 : 99.94 %\n",
      "Epoch 240 : 99.94 %\n",
      "Epoch 250 : 99.94 %\n",
      "Epoch 260 : 99.94 %\n",
      "Epoch 270 : 99.95 %\n",
      "Epoch 280 : 99.95 %\n",
      "Epoch 290 : 99.95 %\n",
      "Epoch 300 : 99.95 %\n",
      "Epoch 310 : 99.95 %\n",
      "Epoch 320 : 99.95 %\n",
      "Epoch 330 : 99.95 %\n",
      "Epoch 340 : 99.95 %\n",
      "Epoch 350 : 99.95 %\n",
      "Epoch 360 : 99.95 %\n",
      "Epoch 370 : 99.95 %\n",
      "Epoch 380 : 99.95 %\n",
      "Epoch 390 : 99.95 %\n",
      "Epoch 400 : 99.95 %\n",
      "Epoch 410 : 99.95 %\n",
      "Epoch 420 : 99.95 %\n",
      "Epoch 430 : 99.95 %\n",
      "Epoch 440 : 99.95 %\n",
      "Epoch 450 : 99.95 %\n",
      "Epoch 460 : 99.95 %\n",
      "Epoch 470 : 99.95 %\n",
      "Epoch 480 : 99.95 %\n",
      "Epoch 490 : 99.95 %\n",
      "Epoch 500 : 99.95 %\n",
      "Epoch 510 : 99.95 %\n",
      "Epoch 520 : 99.95 %\n",
      "Epoch 530 : 99.95 %\n",
      "Epoch 540 : 99.95 %\n",
      "Epoch 550 : 99.95 %\n",
      "Epoch 560 : 99.95 %\n",
      "Epoch 570 : 99.95 %\n",
      "Epoch 580 : 99.95 %\n",
      "Epoch 590 : 99.95 %\n",
      "Epoch 600 : 99.95 %\n",
      "Epoch 610 : 99.95 %\n",
      "Epoch 620 : 99.95 %\n",
      "Epoch 630 : 99.95 %\n",
      "Epoch 640 : 99.95 %\n",
      "Epoch 650 : 99.95 %\n",
      "Epoch 660 : 99.95 %\n",
      "Epoch 670 : 99.95 %\n",
      "Epoch 680 : 99.95 %\n",
      "Epoch 690 : 99.95 %\n",
      "Epoch 700 : 99.95 %\n",
      "Epoch 710 : 99.95 %\n",
      "Epoch 720 : 99.95 %\n",
      "Epoch 730 : 99.95 %\n",
      "Epoch 740 : 99.95 %\n",
      "Epoch 750 : 99.95 %\n",
      "Epoch 760 : 99.95 %\n",
      "Epoch 770 : 99.95 %\n",
      "Epoch 780 : 99.95 %\n",
      "Epoch 790 : 99.95 %\n",
      "Epoch 800 : 99.95 %\n",
      "Epoch 810 : 99.95 %\n",
      "Epoch 820 : 99.95 %\n",
      "Epoch 830 : 99.95 %\n",
      "Epoch 840 : 99.95 %\n",
      "Epoch 850 : 99.95 %\n",
      "Epoch 860 : 99.95 %\n",
      "Epoch 870 : 99.95 %\n",
      "Epoch 880 : 99.95 %\n",
      "Epoch 890 : 99.95 %\n",
      "Epoch 900 : 99.95 %\n",
      "Epoch 910 : 99.95 %\n",
      "Epoch 920 : 99.95 %\n",
      "Epoch 930 : 99.95 %\n",
      "Epoch 940 : 99.95 %\n",
      "Epoch 950 : 99.95 %\n",
      "Epoch 960 : 99.95 %\n",
      "Epoch 970 : 99.95 %\n",
      "Epoch 980 : 99.95 %\n",
      "Epoch 990 : 99.95 %\n",
      "Epoch 1000 : 99.95 %\n",
      "Epoch 1010 : 99.95 %\n",
      "Epoch 1020 : 99.95 %\n",
      "Epoch 1030 : 99.95 %\n",
      "Epoch 1040 : 99.95 %\n",
      "Epoch 1050 : 99.96 %\n",
      "Epoch 1060 : 99.96 %\n",
      "Epoch 1070 : 99.96 %\n",
      "Epoch 1080 : 99.96 %\n",
      "Epoch 1090 : 99.96 %\n",
      "Epoch 1100 : 99.96 %\n",
      "Epoch 1110 : 99.96 %\n",
      "Epoch 1120 : 99.96 %\n",
      "Epoch 1130 : 99.96 %\n",
      "Epoch 1140 : 99.96 %\n",
      "Epoch 1150 : 99.96 %\n",
      "Epoch 1160 : 99.96 %\n",
      "Epoch 1170 : 99.96 %\n",
      "Epoch 1180 : 99.96 %\n",
      "Epoch 1190 : 99.96 %\n",
      "Epoch 1200 : 99.96 %\n",
      "Epoch 1210 : 99.96 %\n",
      "Epoch 1220 : 99.96 %\n",
      "Epoch 1230 : 99.96 %\n",
      "Epoch 1240 : 99.96 %\n",
      "Epoch 1250 : 99.96 %\n",
      "Epoch 1260 : 99.96 %\n",
      "Epoch 1270 : 99.96 %\n",
      "Epoch 1280 : 99.96 %\n",
      "Epoch 1290 : 99.96 %\n",
      "Epoch 1300 : 99.96 %\n",
      "Epoch 1310 : 99.96 %\n",
      "Epoch 1320 : 99.96 %\n",
      "Epoch 1330 : 99.96 %\n",
      "Epoch 1340 : 99.96 %\n",
      "Epoch 1350 : 99.96 %\n",
      "Epoch 1360 : 99.96 %\n",
      "Epoch 1370 : 99.96 %\n",
      "Epoch 1380 : 99.96 %\n",
      "Epoch 1390 : 99.96 %\n",
      "Epoch 1400 : 99.96 %\n",
      "Epoch 1410 : 99.96 %\n",
      "Epoch 1420 : 99.96 %\n",
      "Epoch 1430 : 99.96 %\n",
      "Epoch 1440 : 99.96 %\n",
      "Epoch 1450 : 99.96 %\n",
      "Epoch 1460 : 99.96 %\n",
      "Epoch 1470 : 99.96 %\n",
      "Epoch 1480 : 99.96 %\n",
      "Epoch 1490 : 99.96 %\n",
      "Epoch 1500 : 99.96 %\n",
      "Epoch 1510 : 99.96 %\n",
      "Epoch 1520 : 99.96 %\n",
      "Epoch 1530 : 99.96 %\n",
      "Epoch 1540 : 99.96 %\n",
      "Epoch 1550 : 99.96 %\n",
      "Epoch 1560 : 99.96 %\n",
      "Epoch 1570 : 99.97 %\n",
      "Epoch 1580 : 99.97 %\n",
      "Epoch 1590 : 99.97 %\n",
      "Epoch 1600 : 99.97 %\n",
      "Epoch 1610 : 99.97 %\n",
      "Epoch 1620 : 99.97 %\n",
      "Epoch 1630 : 99.97 %\n",
      "Epoch 1640 : 99.97 %\n",
      "Epoch 1650 : 99.97 %\n",
      "Epoch 1660 : 99.97 %\n",
      "Epoch 1670 : 99.97 %\n",
      "Epoch 1680 : 99.97 %\n",
      "Epoch 1690 : 99.97 %\n",
      "Epoch 1700 : 99.97 %\n",
      "Epoch 1710 : 99.97 %\n",
      "Epoch 1720 : 99.97 %\n",
      "Epoch 1730 : 99.97 %\n",
      "Epoch 1740 : 99.97 %\n",
      "Epoch 1750 : 99.97 %\n",
      "Epoch 1760 : 99.97 %\n",
      "Epoch 1770 : 99.97 %\n",
      "Epoch 1780 : 99.97 %\n",
      "Epoch 1790 : 99.97 %\n",
      "Epoch 1800 : 99.97 %\n",
      "Epoch 1810 : 99.97 %\n",
      "Epoch 1820 : 99.97 %\n",
      "Epoch 1830 : 99.97 %\n",
      "Epoch 1840 : 99.97 %\n",
      "Epoch 1850 : 99.97 %\n",
      "Epoch 1860 : 99.97 %\n",
      "Epoch 1870 : 99.97 %\n",
      "Epoch 1880 : 99.97 %\n",
      "Epoch 1890 : 99.97 %\n",
      "Epoch 1900 : 99.97 %\n",
      "Epoch 1910 : 99.97 %\n",
      "Epoch 1920 : 99.97 %\n",
      "Epoch 1930 : 99.97 %\n",
      "Epoch 1940 : 99.97 %\n",
      "Epoch 1950 : 99.97 %\n",
      "Epoch 1960 : 99.97 %\n",
      "Epoch 1970 : 99.97 %\n",
      "Epoch 1980 : 99.97 %\n",
      "Epoch 1990 : 99.97 %\n",
      "Epoch 2000 : 99.97 %\n",
      "Epoch 2010 : 99.97 %\n",
      "Epoch 2020 : 99.97 %\n",
      "Epoch 2030 : 99.97 %\n",
      "Epoch 2040 : 99.97 %\n",
      "Epoch 2050 : 99.97 %\n",
      "Epoch 2060 : 99.97 %\n",
      "Epoch 2070 : 99.97 %\n",
      "Epoch 2080 : 99.97 %\n",
      "Epoch 2090 : 99.97 %\n",
      "Epoch 2100 : 99.97 %\n",
      "Epoch 2110 : 99.97 %\n",
      "Epoch 2120 : 99.97 %\n",
      "Epoch 2130 : 99.97 %\n",
      "Epoch 2140 : 99.97 %\n",
      "Epoch 2150 : 99.97 %\n",
      "Epoch 2160 : 99.97 %\n",
      "Epoch 2170 : 99.97 %\n",
      "Epoch 2180 : 99.97 %\n",
      "Epoch 2190 : 99.97 %\n",
      "Epoch 2200 : 99.97 %\n",
      "Epoch 2210 : 99.97 %\n",
      "Epoch 2220 : 99.97 %\n",
      "Epoch 2230 : 99.97 %\n",
      "Epoch 2240 : 99.97 %\n",
      "Epoch 2250 : 99.97 %\n",
      "Epoch 2260 : 99.97 %\n",
      "Epoch 2270 : 99.97 %\n",
      "Epoch 2280 : 99.97 %\n",
      "Epoch 2290 : 99.97 %\n",
      "Epoch 2300 : 99.97 %\n",
      "Epoch 2310 : 99.97 %\n",
      "Epoch 2320 : 99.97 %\n",
      "Epoch 2330 : 99.97 %\n",
      "Epoch 2340 : 99.97 %\n",
      "Epoch 2350 : 99.97 %\n",
      "Epoch 2360 : 99.97 %\n",
      "Epoch 2370 : 99.97 %\n",
      "Epoch 2380 : 99.97 %\n",
      "Epoch 2390 : 99.97 %\n",
      "Epoch 2400 : 99.97 %\n",
      "Epoch 2410 : 99.97 %\n",
      "Epoch 2420 : 99.97 %\n",
      "Epoch 2430 : 99.97 %\n",
      "Epoch 2440 : 99.97 %\n",
      "Epoch 2450 : 99.97 %\n",
      "Epoch 2460 : 99.97 %\n",
      "Epoch 2470 : 99.97 %\n",
      "Epoch 2480 : 99.97 %\n",
      "Epoch 2490 : 99.97 %\n",
      "Epoch 2500 : 99.97 %\n",
      "Epoch 2510 : 99.97 %\n",
      "Epoch 2520 : 99.97 %\n",
      "Epoch 2530 : 99.97 %\n",
      "Epoch 2540 : 99.97 %\n",
      "Epoch 2550 : 99.97 %\n",
      "Epoch 2560 : 99.97 %\n",
      "Epoch 2570 : 99.98 %\n",
      "Epoch 2580 : 99.98 %\n",
      "Epoch 2590 : 99.98 %\n",
      "Epoch 2600 : 99.98 %\n",
      "Epoch 2610 : 99.98 %\n",
      "Epoch 2620 : 99.98 %\n",
      "Epoch 2630 : 99.98 %\n",
      "Epoch 2640 : 99.98 %\n",
      "Epoch 2650 : 99.98 %\n",
      "Epoch 2660 : 99.98 %\n",
      "Epoch 2670 : 99.98 %\n",
      "Epoch 2680 : 99.98 %\n",
      "Epoch 2690 : 99.98 %\n",
      "Epoch 2700 : 99.98 %\n",
      "Epoch 2710 : 99.98 %\n",
      "Epoch 2720 : 99.98 %\n",
      "Epoch 2730 : 99.98 %\n",
      "Epoch 2740 : 99.98 %\n",
      "Epoch 2750 : 99.98 %\n",
      "Epoch 2760 : 99.98 %\n",
      "Epoch 2770 : 99.98 %\n",
      "Epoch 2780 : 99.98 %\n",
      "Epoch 2790 : 99.98 %\n",
      "Epoch 2800 : 99.98 %\n",
      "Epoch 2810 : 99.98 %\n",
      "Epoch 2820 : 99.98 %\n",
      "Epoch 2830 : 99.98 %\n",
      "Epoch 2840 : 99.98 %\n",
      "Epoch 2850 : 99.98 %\n",
      "Epoch 2860 : 99.98 %\n",
      "Epoch 2870 : 99.98 %\n",
      "Epoch 2880 : 99.98 %\n",
      "Epoch 2890 : 99.98 %\n",
      "Epoch 2900 : 99.98 %\n",
      "Epoch 2910 : 99.98 %\n",
      "Epoch 2920 : 99.98 %\n",
      "Epoch 2930 : 99.98 %\n",
      "Epoch 2940 : 99.98 %\n",
      "Epoch 2950 : 99.98 %\n",
      "Epoch 2960 : 99.98 %\n",
      "Epoch 2970 : 99.98 %\n",
      "Epoch 2980 : 99.98 %\n",
      "Epoch 2990 : 99.98 %\n",
      "Epoch 3000 : 99.98 %\n",
      "Epoch 3010 : 99.98 %\n",
      "Epoch 3020 : 99.98 %\n",
      "Epoch 3030 : 99.98 %\n",
      "Epoch 3040 : 99.98 %\n",
      "Epoch 3050 : 99.98 %\n",
      "Epoch 3060 : 99.98 %\n",
      "Epoch 3070 : 99.98 %\n",
      "Epoch 3080 : 99.98 %\n",
      "Epoch 3090 : 99.98 %\n",
      "Epoch 3100 : 99.98 %\n",
      "Epoch 3110 : 99.98 %\n",
      "Epoch 3120 : 99.98 %\n",
      "Epoch 3130 : 99.98 %\n",
      "Epoch 3140 : 99.98 %\n",
      "Epoch 3150 : 99.98 %\n",
      "Epoch 3160 : 99.98 %\n",
      "Epoch 3170 : 99.98 %\n",
      "Epoch 3180 : 99.98 %\n",
      "Epoch 3190 : 99.98 %\n",
      "Epoch 3200 : 99.98 %\n",
      "Epoch 3210 : 99.98 %\n",
      "Epoch 3220 : 99.98 %\n",
      "Epoch 3230 : 99.98 %\n",
      "Epoch 3240 : 99.98 %\n",
      "Epoch 3250 : 99.98 %\n",
      "Epoch 3260 : 99.98 %\n",
      "Epoch 3270 : 99.98 %\n",
      "Epoch 3280 : 99.98 %\n",
      "Epoch 3290 : 99.98 %\n",
      "Epoch 3300 : 99.98 %\n",
      "Epoch 3310 : 99.98 %\n",
      "Epoch 3320 : 99.98 %\n",
      "Epoch 3330 : 99.98 %\n",
      "Epoch 3340 : 99.98 %\n",
      "Epoch 3350 : 99.98 %\n",
      "Epoch 3360 : 99.98 %\n",
      "Epoch 3370 : 99.98 %\n",
      "Epoch 3380 : 99.98 %\n",
      "Epoch 3390 : 99.98 %\n",
      "Epoch 3400 : 99.98 %\n",
      "Epoch 3410 : 99.98 %\n",
      "Epoch 3420 : 99.98 %\n",
      "Epoch 3430 : 99.98 %\n",
      "Epoch 3440 : 99.98 %\n",
      "Epoch 3450 : 99.98 %\n",
      "Epoch 3460 : 99.98 %\n",
      "Epoch 3470 : 99.98 %\n",
      "Epoch 3480 : 99.98 %\n",
      "Epoch 3490 : 99.98 %\n",
      "Epoch 3500 : 99.98 %\n",
      "Epoch 3510 : 99.98 %\n",
      "Epoch 3520 : 99.98 %\n",
      "Epoch 3530 : 99.98 %\n",
      "Epoch 3540 : 99.99 %\n",
      "Epoch 3550 : 99.99 %\n",
      "Epoch 3560 : 99.99 %\n",
      "Epoch 3570 : 99.99 %\n",
      "Epoch 3580 : 99.99 %\n",
      "Epoch 3590 : 99.99 %\n",
      "Epoch 3600 : 99.99 %\n",
      "Epoch 3610 : 99.99 %\n",
      "Epoch 3620 : 99.99 %\n",
      "Epoch 3630 : 99.99 %\n",
      "Epoch 3640 : 99.99 %\n",
      "Epoch 3650 : 99.99 %\n",
      "Epoch 3660 : 99.99 %\n",
      "Epoch 3670 : 99.99 %\n",
      "Epoch 3680 : 99.99 %\n",
      "Epoch 3690 : 99.99 %\n",
      "Epoch 3700 : 99.99 %\n",
      "Epoch 3710 : 99.99 %\n",
      "Epoch 3720 : 99.99 %\n",
      "Epoch 3730 : 99.99 %\n",
      "Epoch 3740 : 99.99 %\n",
      "Epoch 3750 : 99.99 %\n",
      "Epoch 3760 : 99.99 %\n",
      "Epoch 3770 : 99.99 %\n",
      "Epoch 3780 : 99.99 %\n",
      "Epoch 3790 : 99.99 %\n",
      "Epoch 3800 : 99.99 %\n",
      "Epoch 3810 : 99.99 %\n",
      "Epoch 3820 : 99.99 %\n",
      "Epoch 3830 : 99.99 %\n",
      "Epoch 3840 : 99.99 %\n",
      "Epoch 3850 : 99.99 %\n",
      "Epoch 3860 : 99.99 %\n",
      "Epoch 3870 : 99.99 %\n",
      "Epoch 3880 : 99.99 %\n",
      "Epoch 3890 : 99.99 %\n",
      "Epoch 3900 : 99.99 %\n",
      "Epoch 3910 : 99.99 %\n",
      "Epoch 3920 : 99.99 %\n",
      "Epoch 3930 : 99.99 %\n",
      "Epoch 3940 : 99.99 %\n",
      "Epoch 3950 : 99.99 %\n",
      "Epoch 3960 : 99.99 %\n",
      "Epoch 3970 : 99.99 %\n",
      "Epoch 3980 : 99.99 %\n",
      "Epoch 3990 : 99.99 %\n",
      "Epoch 4000 : 99.99 %\n",
      "Epoch 4010 : 99.99 %\n",
      "Epoch 4020 : 99.99 %\n",
      "Epoch 4030 : 99.99 %\n",
      "Epoch 4040 : 99.99 %\n",
      "Epoch 4050 : 99.99 %\n",
      "Epoch 4060 : 99.99 %\n",
      "Epoch 4070 : 99.99 %\n",
      "Epoch 4080 : 99.99 %\n",
      "Epoch 4090 : 99.99 %\n",
      "Epoch 4100 : 99.99 %\n",
      "Epoch 4110 : 99.99 %\n",
      "Epoch 4120 : 99.99 %\n",
      "Epoch 4130 : 99.99 %\n",
      "Epoch 4140 : 99.99 %\n",
      "Epoch 4150 : 99.99 %\n",
      "Epoch 4160 : 99.99 %\n",
      "Epoch 4170 : 99.99 %\n",
      "Epoch 4180 : 99.99 %\n",
      "Epoch 4190 : 99.99 %\n",
      "Epoch 4200 : 99.99 %\n",
      "Epoch 4210 : 99.99 %\n",
      "Epoch 4220 : 99.99 %\n",
      "Epoch 4230 : 99.99 %\n",
      "Epoch 4240 : 99.99 %\n",
      "Epoch 4250 : 99.99 %\n",
      "Epoch 4260 : 99.99 %\n",
      "Epoch 4270 : 99.99 %\n",
      "Epoch 4280 : 99.99 %\n",
      "Epoch 4290 : 99.99 %\n",
      "Epoch 4300 : 99.99 %\n",
      "Epoch 4310 : 99.99 %\n",
      "Epoch 4320 : 99.99 %\n",
      "Epoch 4330 : 99.99 %\n",
      "Epoch 4340 : 99.99 %\n",
      "Epoch 4350 : 99.99 %\n",
      "Epoch 4360 : 99.99 %\n",
      "Epoch 4370 : 99.99 %\n",
      "Epoch 4380 : 99.99 %\n",
      "Epoch 4390 : 99.99 %\n",
      "Epoch 4400 : 99.99 %\n",
      "Epoch 4410 : 99.99 %\n",
      "Epoch 4420 : 99.99 %\n",
      "Epoch 4430 : 99.99 %\n",
      "Epoch 4440 : 99.99 %\n",
      "Epoch 4450 : 99.99 %\n",
      "Epoch 4460 : 99.99 %\n",
      "Epoch 4470 : 99.99 %\n",
      "Epoch 4480 : 99.99 %\n",
      "Epoch 4490 : 99.99 %\n",
      "Epoch 4500 : 99.99 %\n",
      "Epoch 4510 : 99.99 %\n",
      "Epoch 4520 : 99.99 %\n",
      "Epoch 4530 : 99.99 %\n",
      "Epoch 4540 : 99.99 %\n",
      "Epoch 4550 : 99.99 %\n",
      "Epoch 4560 : 99.99 %\n",
      "Epoch 4570 : 99.99 %\n",
      "Epoch 4580 : 99.99 %\n",
      "Epoch 4590 : 99.99 %\n",
      "Epoch 4600 : 99.99 %\n",
      "Epoch 4610 : 99.99 %\n",
      "Epoch 4620 : 99.99 %\n",
      "Epoch 4630 : 99.99 %\n",
      "Epoch 4640 : 99.99 %\n",
      "Epoch 4650 : 99.99 %\n",
      "Epoch 4660 : 99.99 %\n",
      "Epoch 4670 : 99.99 %\n",
      "Epoch 4680 : 99.99 %\n",
      "Epoch 4690 : 99.99 %\n",
      "Epoch 4700 : 99.99 %\n",
      "Epoch 4710 : 99.99 %\n",
      "Epoch 4720 : 99.99 %\n",
      "Epoch 4730 : 99.99 %\n",
      "Epoch 4740 : 99.99 %\n",
      "Epoch 4750 : 99.99 %\n",
      "Epoch 4760 : 99.99 %\n",
      "Epoch 4770 : 99.99 %\n",
      "Epoch 4780 : 99.99 %\n",
      "Epoch 4790 : 99.99 %\n",
      "Epoch 4800 : 99.99 %\n",
      "Epoch 4810 : 99.99 %\n",
      "Epoch 4820 : 99.99 %\n",
      "Epoch 4830 : 99.99 %\n",
      "Epoch 4840 : 99.99 %\n",
      "Epoch 4850 : 99.99 %\n",
      "Epoch 4860 : 99.99 %\n",
      "Epoch 4870 : 99.99 %\n",
      "Epoch 4880 : 99.99 %\n",
      "Epoch 4890 : 99.99 %\n",
      "Epoch 4900 : 99.99 %\n",
      "Epoch 4910 : 99.99 %\n",
      "Epoch 4920 : 99.99 %\n",
      "Epoch 4930 : 99.99 %\n",
      "Epoch 4940 : 99.99 %\n",
      "Epoch 4950 : 99.99 %\n",
      "Epoch 4960 : 99.99 %\n",
      "Epoch 4970 : 99.99 %\n",
      "Epoch 4980 : 99.99 %\n",
      "Epoch 4990 : 99.99 %\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "nn.set_learning_rate(0.05)\n",
    "nn.train_GD(5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
