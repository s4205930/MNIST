{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "853a9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MNIST.csv')\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f95c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape\n",
    "\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:n]\n",
    "X_test = X_test / 255\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255\n",
    "\n",
    "_, m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "025dbe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 41000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9031288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size, learning_rate, data_num, bias = 1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.m = data_num\n",
    "\n",
    "        # self.W1 = np.random.rand(hidden_1_size, input_size) - 0.5\n",
    "        # self.b1 = np.random.rand(hidden_1_size, bias) - 0.5\n",
    "\n",
    "        # self.W2 = np.random.rand(hidden_2_size, hidden_1_size) - 0.5\n",
    "        # self.b2 = np.random.rand(hidden_2_size, bias) - 0.5\n",
    "\n",
    "        # self.W3 = np.random.rand(output_size, hidden_2_size) - 0.5\n",
    "        # self.b3 = np.random.rand(output_size, bias) - 0.5\n",
    "\n",
    "        # Xavier initialization for weights\n",
    "        self.W1 = np.random.randn(hidden_1_size, input_size) * np.sqrt(1.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_1_size, bias))\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_2_size, hidden_1_size) * np.sqrt(1.0 / hidden_1_size)\n",
    "        self.b2 = np.zeros((hidden_2_size, bias))\n",
    "\n",
    "        self.W3 = np.random.randn(output_size, hidden_2_size) * np.sqrt(1.0 / hidden_2_size)\n",
    "        self.b3 = np.zeros((output_size, bias))\n",
    "\n",
    "    def set_learning_rate(self, new_LR):\n",
    "        self.learning_rate = new_LR\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        self.Z1 = self.W1.dot(X) + self.b1\n",
    "        self.A1 = ReLU(self.Z1)\n",
    "\n",
    "        self.Z2 = self.W2.dot(self.A1) + self.b2\n",
    "        self.A2 = ReLU(self.Z2)\n",
    "\n",
    "        self.Z3 = self.W3.dot(self.A2) + self.b3\n",
    "        self.A3 = softmax(self.Z3)\n",
    "\n",
    "        return self.A3, self.Z3\n",
    "\n",
    "    def backward_prop(self, X, y):\n",
    "        one_hot_Y = one_hot(y)\n",
    "\n",
    "        self.dZ3 = self.A3 - one_hot_Y\n",
    "        self.dW3 = 1 / self.m * self.dZ3.dot(self.A2.T)\n",
    "        self.db3 = 1 / self.m * np.sum(self.dZ3, axis = 1, keepdims = True)\n",
    "\n",
    "        self.dZ2 = self.W3.T.dot(self.dZ3) * ReLU_prime(self.Z2)\n",
    "        self.dW2 = 1 / self.m * self.dZ2.dot(self.A1.T)\n",
    "        self.db2 = 1 / self.m * np.sum(self.dZ2, axis = 1, keepdims = True)\n",
    "\n",
    "        self.dZ1 = self.W2.T.dot(self.dZ2) * ReLU_prime(self.Z1)\n",
    "        self.dW1 = 1 / self.m * self.dZ1.dot(X.T)\n",
    "        self.db1 = 1 / self.m * np.sum(self.dZ1, axis = 1, keepdims = True)\n",
    "\n",
    "        #return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        self.W1 = self.W1 - self.dW1 * self.learning_rate\n",
    "        self.b1 = self.b1 - self.db1 * self.learning_rate\n",
    "\n",
    "        self.W2 = self.W2 - self.dW2 * self.learning_rate\n",
    "        self.b2 = self.b2 - self.db2 * self.learning_rate\n",
    "\n",
    "        self.W3 = self.W3 - self.dW3 * self.learning_rate\n",
    "        self.b3 = self.b3 - self.db3 * self.learning_rate\n",
    "\n",
    "\n",
    "    def train_GD(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "            self.gradient_descent()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                predictions = np.argmax(self.A3, 0)\n",
    "                print(\"Epoch\", epoch, \":\", round((np.sum(predictions == Y_train) / Y_train.size) * 100, 10), \"%\")\n",
    "        print(\"Training complete\")           \n",
    "        \n",
    "\n",
    "class PReLU:\n",
    "    def __init__(self, alpha_init = 0.01):\n",
    "        self.alpha = alpha_init\n",
    "\n",
    "    def forward(self, Z):\n",
    "        return np.maximum(self.alpha * Z, Z)\n",
    "\n",
    "    def prime(self, Z):\n",
    "        return np.where(Z > 0, 1, self.alpha)\n",
    "\n",
    "def update_alpha(alpha, dZ):\n",
    "    learning_rate = 0.01\n",
    "    alpha -= learning_rate * np.mean(np.where(dZ < 0, dZ * alpha, 0))\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_prime(Z):\n",
    "    return Z > 0\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(np.clip(-Z, -4, 4)))\n",
    "    return A\n",
    "\n",
    "def sigmoid_prime(Z):\n",
    "    A = (sigmoid(Z) * (1 - sigmoid(Z)))\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b208f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(784, 64, 32, 10, 0.05, 41000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3950c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 6.812195122 %\n",
      "Epoch 100 : 81.4048780488 %\n",
      "Epoch 200 : 87.2024390244 %\n",
      "Epoch 300 : 89.3073170732 %\n",
      "Epoch 400 : 90.4195121951 %\n",
      "Epoch 500 : 91.0975609756 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m nn\u001b[39m.\u001b[39mset_learning_rate(\u001b[39m0.05\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m nn\u001b[39m.\u001b[39;49mtrain_GD(\u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[21], line 75\u001b[0m, in \u001b[0;36mNeural_Network.train_GD\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_GD\u001b[39m(\u001b[39mself\u001b[39m, epochs):\n\u001b[0;32m     73\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> 75\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_prop(X_train)\n\u001b[0;32m     76\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_prop(X_train, Y_train)\n\u001b[0;32m     77\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_descent()\n",
      "Cell \u001b[1;32mIn[21], line 33\u001b[0m, in \u001b[0;36mNeural_Network.forward_prop\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_prop\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mZ1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW1\u001b[39m.\u001b[39;49mdot(X) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb1\n\u001b[0;32m     34\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA1 \u001b[39m=\u001b[39m ReLU(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mZ1)\n\u001b[0;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mZ2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW2\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mA1) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb2\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn.set_learning_rate(0.05)\n",
    "nn.train_GD(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe77e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
