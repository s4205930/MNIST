{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853a9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MNIST.csv')\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape\n",
    "\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:n]\n",
    "X_test = X_test / 255\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255\n",
    "\n",
    "_, m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025dbe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 41000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9031288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size, learning_rate, data_num, bias = 1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.m = data_num\n",
    "\n",
    "        # self.W1 = np.random.rand(hidden_1_size, input_size) - 0.5\n",
    "        # self.b1 = np.random.rand(hidden_1_size, bias) - 0.5\n",
    "\n",
    "        # self.W2 = np.random.rand(hidden_2_size, hidden_1_size) - 0.5\n",
    "        # self.b2 = np.random.rand(hidden_2_size, bias) - 0.5\n",
    "\n",
    "        # self.W3 = np.random.rand(output_size, hidden_2_size) - 0.5\n",
    "        # self.b3 = np.random.rand(output_size, bias) - 0.5\n",
    "\n",
    "        # Xavier initialization for weights\n",
    "        self.W1 = np.random.randn(hidden_1_size, input_size) * np.sqrt(1.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_1_size, bias))\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_2_size, hidden_1_size) * np.sqrt(1.0 / hidden_1_size)\n",
    "        self.b2 = np.zeros((hidden_2_size, bias))\n",
    "\n",
    "        self.W3 = np.random.randn(output_size, hidden_2_size) * np.sqrt(1.0 / hidden_2_size)\n",
    "        self.b3 = np.zeros((output_size, bias))\n",
    "\n",
    "    def set_learning_rate(self, new_LR):\n",
    "        self.learning_rate = new_LR\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        self.Z1 = self.W1.dot(X) + self.b1\n",
    "        self.A1 = ReLU(self.Z1)\n",
    "\n",
    "        self.Z2 = self.W2.dot(self.A1) + self.b2\n",
    "        self.A2 = ReLU(self.Z2)\n",
    "\n",
    "        self.Z3 = self.W3.dot(self.A2) + self.b3\n",
    "        self.A3 = softmax(self.Z3)\n",
    "\n",
    "        return self.A3, self.Z3\n",
    "\n",
    "    def backward_prop(self, X, y):\n",
    "        one_hot_Y = one_hot(y)\n",
    "\n",
    "        # print(\"Shape of self.A3:\", self.A3.shape)\n",
    "        # print(\"Shape of one_hot_Y:\", one_hot_Y.shape)\n",
    "\n",
    "\n",
    "        self.dZ3 = self.A3 - one_hot_Y\n",
    "        self.dW3 = 1 / self.m * self.dZ3.dot(self.A2.T)\n",
    "        self.db3 = 1 / self.m * np.sum(self.dZ3, axis = 1, keepdims = True)\n",
    "\n",
    "        self.dZ2 = self.W3.T.dot(self.dZ3) * ReLU_prime(self.Z2)\n",
    "        self.dW2 = 1 / self.m * self.dZ2.dot(self.A1.T)\n",
    "        self.db2 = 1 / self.m * np.sum(self.dZ2, axis = 1, keepdims = True)\n",
    "\n",
    "        self.dZ1 = self.W2.T.dot(self.dZ2) * ReLU_prime(self.Z1)\n",
    "        self.dW1 = 1 / self.m * self.dZ1.dot(X.T)\n",
    "        self.db1 = 1 / self.m * np.sum(self.dZ1, axis = 1, keepdims = True)\n",
    "\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        self.W1 = self.W1 - self.dW1 * self.learning_rate\n",
    "        self.b1 = self.b1 - self.db1 * self.learning_rate\n",
    "\n",
    "        self.W2 = self.W2 - self.dW2 * self.learning_rate\n",
    "        self.b2 = self.b2 - self.db2 * self.learning_rate\n",
    "\n",
    "        self.W3 = self.W3 - self.dW3 * self.learning_rate\n",
    "        self.b3 = self.b3 - self.db3 * self.learning_rate\n",
    "\n",
    "\n",
    "    def train_GD(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "            self.gradient_descent()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "    def train_SGD(self, epochs, batch = 64):\n",
    "        num_samples = X_train.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indicies = np.random.permutation(num_samples)\n",
    "            X_train_shuffled = X_train[:, indicies]\n",
    "            Y_train_shuffled = Y_train[indicies]\n",
    "\n",
    "            for i in range(0, num_samples, batch):\n",
    "                X_batch = X_train_shuffled[:, i:i+batch]\n",
    "                Y_batch = Y_train_shuffled[i:i+batch]\n",
    "\n",
    "                self.forward_prop(X_batch)\n",
    "                self.backward_prop(X_batch, Y_batch)\n",
    "                self.gradient_descent()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "    def train_adam(self, epochs):\n",
    "        #Init Extra Weights\n",
    "        self.beta1 = 0.9 # Exp decay rate for mean of gradients\n",
    "        self.beta2 = 0.999 # Exp decay rate for varience of gradients\n",
    "        self.epsilon = 1e-8 # Prevent divisions by 0\n",
    "        self.m_W1 = np.zeros_like(self.W1) # Moving average of gradients\n",
    "        self.v_W1 = np.zeros_like(self.W1) # Squared moving averages of gradients\n",
    "        self.m_b1 = np.zeros_like(self.b1)\n",
    "        self.v_b1 = np.zeros_like(self.b1)\n",
    "        self.m_W2 = np.zeros_like(self.W2)\n",
    "        self.v_W2 = np.zeros_like(self.W2)\n",
    "        self.m_b2 = np.zeros_like(self.b2)\n",
    "        self.v_b2 = np.zeros_like(self.b2)\n",
    "        self.m_W3 = np.zeros_like(self.W3)\n",
    "        self.v_W3 = np.zeros_like(self.W3)\n",
    "        self.m_b3 = np.zeros_like(self.b3)\n",
    "        self.v_b3 = np.zeros_like(self.b3)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "            self.adam_update()\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "    def adam_update(self):\n",
    "        self.m_W1 = self.beta1 * self.m_W1 + (1 - self.beta1) * self.dW1\n",
    "        self.v_W1 = self.beta2 * self.v_W1 + (1 - self.beta2) * (self.dW1 ** 2)\n",
    "        m_W1_hat = self.m_W1 / (1 - self.beta1)\n",
    "        v_W1_hat = self.v_W1 / (1 - self.beta2)\n",
    "        self.W1 -= self.learning_rate * m_W1_hat / (np.sqrt(v_W1_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b1 = self.beta1 * self.m_b1 + (1 - self.beta1) * self.db1\n",
    "        self.v_b1 = self.beta2 * self.v_b1 + (1 - self.beta2) * (self.db1 ** 2)\n",
    "        m_b1_hat = self.m_b1 / (1 - self.beta1)\n",
    "        v_b1_hat = self.v_b1 / (1 - self.beta2)\n",
    "        self.b1 -= self.learning_rate * m_b1_hat / (np.sqrt(v_b1_hat) + self.epsilon)\n",
    "\n",
    "        self.m_W2 = self.beta1 * self.m_W2 + (1 - self.beta1) * self.dW2\n",
    "        self.v_W2 = self.beta2 * self.v_W2 + (1 - self.beta2) * (self.dW2 ** 2)\n",
    "        m_W2_hat = self.m_W2 / (1 - self.beta1)\n",
    "        v_W2_hat = self.v_W2 / (1 - self.beta2)\n",
    "        self.W2 -= self.learning_rate * m_W2_hat / (np.sqrt(v_W2_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b2 = self.beta1 * self.m_b2 + (1 - self.beta1) * self.db2\n",
    "        self.v_b2 = self.beta2 * self.v_b2 + (1 - self.beta2) * (self.db2 ** 2)\n",
    "        m_b2_hat = self.m_b2 / (1 - self.beta1)\n",
    "        v_b2_hat = self.v_b2 / (1 - self.beta2)\n",
    "        self.b2 -= self.learning_rate * m_b2_hat / (np.sqrt(v_b2_hat) + self.epsilon)\n",
    "\n",
    "        self.m_W3 = self.beta1 * self.m_W3 + (1 - self.beta1) * self.dW3\n",
    "        self.v_W3 = self.beta2 * self.v_W3 + (1 - self.beta2) * (self.dW3 ** 2)\n",
    "        m_W3_hat = self.m_W3 / (1 - self.beta1)\n",
    "        v_W3_hat = self.v_W3 / (1 - self.beta2)\n",
    "        self.W3 -= self.learning_rate * m_W3_hat / (np.sqrt(v_W3_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b3 = self.beta1 * self.m_b3 + (1 - self.beta1) * self.db3\n",
    "        self.v_b3 = self.beta2 * self.v_b3 + (1 - self.beta2) * (self.db3 ** 2)\n",
    "        m_b3_hat = self.m_b3 / (1 - self.beta1)\n",
    "        v_b3_hat = self.v_b3 / (1 - self.beta2)\n",
    "        self.b3 -= self.learning_rate * m_b3_hat / (np.sqrt(v_b3_hat) + self.epsilon)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, method, epochs):\n",
    "        if method == 'GD':\n",
    "            self.train_GD(epochs)\n",
    "        elif method == 'SGD':\n",
    "            self.train_SGD(epochs)\n",
    "        elif method == 'adam':\n",
    "            self.train_adam(epochs)\n",
    "        else:\n",
    "            raise ValueError('Invalid Training Method')\n",
    "            # Add more cases for different training methods\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class PReLU:\n",
    "    def __init__(self, alpha_init = 0.01):\n",
    "        self.alpha = alpha_init\n",
    "\n",
    "    def forward(self, Z):\n",
    "        return np.maximum(self.alpha * Z, Z)\n",
    "\n",
    "    def prime(self, Z):\n",
    "        return np.where(Z > 0, 1, self.alpha)\n",
    "\n",
    "def update_alpha(alpha, dZ):\n",
    "    learning_rate = 0.01\n",
    "    alpha -= learning_rate * np.mean(np.where(dZ < 0, dZ * alpha, 0))\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_prime(Z):\n",
    "    return Z > 0\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(np.clip(-Z, -4, 4)))\n",
    "    return A\n",
    "\n",
    "def sigmoid_prime(Z):\n",
    "    A = (sigmoid(Z) * (1 - sigmoid(Z)))\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, 10))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b208f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(784, 64, 32, 10, 0.05, 41000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3950c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 99.1829%\n",
      "Epoch 100: 99.2634%\n",
      "Epoch 200: 99.3%\n",
      "Epoch 300: 99.3244%\n",
      "Epoch 400: 99.3512%\n",
      "Epoch 500: 99.3732%\n",
      "Epoch 600: 99.4146%\n",
      "Epoch 700: 99.4512%\n",
      "Epoch 800: 99.4659%\n",
      "Epoch 900: 99.4805%\n",
      "Training complete\n",
      "\n",
      "Test Set Accuracy: 92.3%\n"
     ]
    }
   ],
   "source": [
    "nn.set_learning_rate(0.001)\n",
    "nn.train('adam', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe77e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
