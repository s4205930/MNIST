{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import numpy library for numerical computations\n",
    "import pandas as pd  # Import pandas library for data manipulation and analysis\n",
    "from pyDOE import lhs  # Import lhs function from pyDOE module for Latin Hypercube Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MNIST.csv')  # Read the data from the 'MNIST.csv' file into a pandas DataFrame\n",
    "data = np.array(data)  # Convert the DataFrame to a numpy array\n",
    "np.random.shuffle(data)  # Shuffle the rows of the data array randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape  # Get the dimensions of the data array\n",
    "\n",
    "# Extract the first 1000 rows for testing data and transpose them\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]  # Extract the labels for testing data\n",
    "X_test = data_test[1:n]  # Extract the features for testing data\n",
    "X_test = X_test / 255  # Normalise the features by dividing by 255 (maximum pixel value)\n",
    "\n",
    "# Extract the remaining rows for training data and transpose them\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]  # Extract the labels for training data\n",
    "X_train = data_train[1:n]  # Extract the features for training data\n",
    "X_train = X_train / 255  # Normalise the features by dividing by 255 (maximum pixel value)\n",
    "\n",
    "_, m_train = X_train.shape  # Get the number of training examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dbe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape  # Check the shape of the training data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9031288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size, learning_rate, data_num, bias = 1):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with given parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size: Number of input features.\n",
    "        - hidden_1_size: Number of neurons in the first hidden layer.\n",
    "        - hidden_2_size: Number of neurons in the second hidden layer.\n",
    "        - output_size: Number of output classes.\n",
    "        - learning_rate: Learning rate for gradient descent.\n",
    "        - data_num: Number of data points in the dataset.\n",
    "        - bias: Bias term (default is 1).\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.m = data_num\n",
    "\n",
    "        # Xavier initialization for weights\n",
    "        self.W1 = np.random.randn(hidden_1_size, input_size) * np.sqrt(1.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_1_size, bias))\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_2_size, hidden_1_size) * np.sqrt(1.0 / hidden_1_size)\n",
    "        self.b2 = np.zeros((hidden_2_size, bias))\n",
    "\n",
    "        self.W3 = np.random.randn(output_size, hidden_2_size) * np.sqrt(1.0 / hidden_2_size)\n",
    "        self.b3 = np.zeros((output_size, bias))\n",
    "\n",
    "    def set_learning_rate(self, new_LR):\n",
    "        \"\"\"\n",
    "        Update the learning rate.\n",
    "\n",
    "        Parameters:\n",
    "        - new_LR: New learning rate value.\n",
    "        \"\"\"\n",
    "        self.learning_rate = new_LR\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data matrix.\n",
    "\n",
    "        Returns:\n",
    "        - A3: Output predictions after the final activation function.\n",
    "        - Z3: Output of the final layer before the activation function.\n",
    "        \"\"\"\n",
    "        self.Z1 = self.W1.dot(X) + self.b1\n",
    "        self.A1 = ReLU(self.Z1)\n",
    "\n",
    "        self.Z2 = self.W2.dot(self.A1) + self.b2\n",
    "        self.A2 = ReLU(self.Z2)\n",
    "\n",
    "        self.Z3 = self.W3.dot(self.A2) + self.b3\n",
    "        self.A3 = softmax(self.Z3)\n",
    "\n",
    "        return self.A3, self.Z3\n",
    "\n",
    "    def backward_prop(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform backward propagation to compute gradients.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data matrix.\n",
    "        - y: True labels.\n",
    "        \"\"\"\n",
    "        one_hot_Y = one_hot(y)\n",
    "\n",
    "        self.dZ3 = self.A3 - one_hot_Y\n",
    "        self.dW3 = 1 / self.m * self.dZ3.dot(self.A2.T)\n",
    "        self.db3 = 1 / self.m * np.sum(self.dZ3, axis=1, keepdims=True)\n",
    "\n",
    "        self.dZ2 = self.W3.T.dot(self.dZ3) * ReLU_prime(self.Z2)\n",
    "        self.dW2 = 1 / self.m * self.dZ2.dot(self.A1.T)\n",
    "        self.db2 = 1 / self.m * np.sum(self.dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        self.dZ1 = self.W2.T.dot(self.dZ2) * ReLU_prime(self.Z1)\n",
    "        self.dW1 = 1 / self.m * self.dZ1.dot(X.T)\n",
    "        self.db1 = 1 / self.m * np.sum(self.dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        \"\"\"\n",
    "        Update weights using gradient descent.\n",
    "        \"\"\"\n",
    "        self.W1 = self.W1 - self.dW1 * self.learning_rate\n",
    "        self.b1 = self.b1 - self.db1 * self.learning_rate\n",
    "\n",
    "        self.W2 = self.W2 - self.dW2 * self.learning_rate\n",
    "        self.b2 = self.b2 - self.db2 * self.learning_rate\n",
    "\n",
    "        self.W3 = self.W3 - self.dW3 * self.learning_rate\n",
    "        self.b3 = self.b3 - self.db3 * self.learning_rate\n",
    "\n",
    "    def train_GD(self, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using batch gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "            self.gradient_descent()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "\n",
    "    def train_SGD(self, epochs, batch = 128):\n",
    "        num_samples = X_train.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indicies = np.random.permutation(num_samples)\n",
    "            X_train_shuffled = X_train[:, indicies]\n",
    "            Y_train_shuffled = Y_train[indicies]\n",
    "\n",
    "            for i in range(0, num_samples, batch):\n",
    "                X_batch = X_train_shuffled[:, i:i+batch]\n",
    "                Y_batch = Y_train_shuffled[i:i+batch]\n",
    "\n",
    "                self.forward_prop(X_batch)\n",
    "                self.backward_prop(X_batch, Y_batch)\n",
    "                self.gradient_descent()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "    def train_adam(self, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using the Adam optimization algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"        \n",
    "        #Init Extra Weights\n",
    "        self.beta1 = 0.6 # Exp decay rate for mean of gradients\n",
    "        self.beta2 = 0.6 # Exp decay rate for varience of gradients\n",
    "        self.epsilon = 1e-8 # Prevent divisions by 0\n",
    "        self.m_W1 = np.zeros_like(self.W1) # Moving average of gradients\n",
    "        self.v_W1 = np.zeros_like(self.W1) # Squared moving averages of gradients\n",
    "        self.m_b1 = np.zeros_like(self.b1)\n",
    "        self.v_b1 = np.zeros_like(self.b1)\n",
    "        self.m_W2 = np.zeros_like(self.W2)\n",
    "        self.v_W2 = np.zeros_like(self.W2)\n",
    "        self.m_b2 = np.zeros_like(self.b2)\n",
    "        self.v_b2 = np.zeros_like(self.b2)\n",
    "        self.m_W3 = np.zeros_like(self.W3)\n",
    "        self.v_W3 = np.zeros_like(self.W3)\n",
    "        self.m_b3 = np.zeros_like(self.b3)\n",
    "        self.v_b3 = np.zeros_like(self.b3)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Perform forward and backward propagation\n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "\n",
    "            # Update parameters using Adam optimization rule\n",
    "            self.adam_update()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                # Evaluate and print accuracy every 10 epochs\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)\n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        # Evaluate accuracy on test set after training\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "    def adam_update(self):\n",
    "        \"\"\"\n",
    "        Update weights and biases using the Adam optimization algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update the first moment estimate of gradients for weights W1 using exponential decay\n",
    "        self.m_W1 = self.beta1 * self.m_W1 + (1 - self.beta1) * self.dW1\n",
    "\n",
    "        # Update the second moment estimate of gradients for weights W1 using exponential decay\n",
    "        self.v_W1 = self.beta2 * self.v_W1 + (1 - self.beta2) * (self.dW1 ** 2)\n",
    "\n",
    "        # Correct bias in the first moment estimate\n",
    "        m_W1_hat = self.m_W1 / (1 - self.beta1)\n",
    "\n",
    "        # Correct bias in the second moment estimate\n",
    "        v_W1_hat = self.v_W1 / (1 - self.beta2)\n",
    "\n",
    "        # Update weights W1 using the Adam optimization update rule\n",
    "        self.W1 -= self.learning_rate * m_W1_hat / (np.sqrt(v_W1_hat) + self.epsilon)\n",
    "\n",
    "        #Repeat for following weights and biases\n",
    "\n",
    "        self.m_b1 = self.beta1 * self.m_b1 + (1 - self.beta1) * self.db1\n",
    "        self.v_b1 = self.beta2 * self.v_b1 + (1 - self.beta2) * (self.db1 ** 2)\n",
    "        m_b1_hat = self.m_b1 / (1 - self.beta1)\n",
    "        v_b1_hat = self.v_b1 / (1 - self.beta2)\n",
    "        self.b1 -= self.learning_rate * m_b1_hat / (np.sqrt(v_b1_hat) + self.epsilon)\n",
    "\n",
    "        self.m_W2 = self.beta1 * self.m_W2 + (1 - self.beta1) * self.dW2\n",
    "        self.v_W2 = self.beta2 * self.v_W2 + (1 - self.beta2) * (self.dW2 ** 2)\n",
    "        m_W2_hat = self.m_W2 / (1 - self.beta1)\n",
    "        v_W2_hat = self.v_W2 / (1 - self.beta2)\n",
    "        self.W2 -= self.learning_rate * m_W2_hat / (np.sqrt(v_W2_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b2 = self.beta1 * self.m_b2 + (1 - self.beta1) * self.db2\n",
    "        self.v_b2 = self.beta2 * self.v_b2 + (1 - self.beta2) * (self.db2 ** 2)\n",
    "        m_b2_hat = self.m_b2 / (1 - self.beta1)\n",
    "        v_b2_hat = self.v_b2 / (1 - self.beta2)\n",
    "        self.b2 -= self.learning_rate * m_b2_hat / (np.sqrt(v_b2_hat) + self.epsilon)\n",
    "\n",
    "        self.m_W3 = self.beta1 * self.m_W3 + (1 - self.beta1) * self.dW3\n",
    "        self.v_W3 = self.beta2 * self.v_W3 + (1 - self.beta2) * (self.dW3 ** 2)\n",
    "        m_W3_hat = self.m_W3 / (1 - self.beta1)\n",
    "        v_W3_hat = self.v_W3 / (1 - self.beta2)\n",
    "        self.W3 -= self.learning_rate * m_W3_hat / (np.sqrt(v_W3_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b3 = self.beta1 * self.m_b3 + (1 - self.beta1) * self.db3\n",
    "        self.v_b3 = self.beta2 * self.v_b3 + (1 - self.beta2) * (self.db3 ** 2)\n",
    "        m_b3_hat = self.m_b3 / (1 - self.beta1)\n",
    "        v_b3_hat = self.v_b3 / (1 - self.beta2)\n",
    "        self.b3 -= self.learning_rate * m_b3_hat / (np.sqrt(v_b3_hat) + self.epsilon)\n",
    "\n",
    "    \n",
    "   \n",
    "    def train_pso(self, epochs, n_particles=100, w_start=0.9, w_end=0.3, c1=0.5, c2=0.3, max_velocity=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using Particle Swarm Optimization (PSO).\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        - n_particles: Number of particles (default is 50).\n",
    "        - w_start: Initial inertia weight (default is 0.9).\n",
    "        - w_end: Final inertia weight (default is 0.2).\n",
    "        - c1: Cognitive parameter (default is 1.5).\n",
    "        - c2: Social parameter (default is 1.5).\n",
    "        - max_velocity: Maximum velocity for particle movement (default is None).\n",
    "        \"\"\"\n",
    "        # Initialize particles' positions using Latin Hypercube Sampling\n",
    "        particle_positions = self._latin_hypercube_sampling(n_particles)\n",
    "        # Initialize particles' velocities\n",
    "        particle_velocities = [np.random.randn(len(particle_positions[0])) * 0.1 for _ in range(n_particles)]  # Adjust the scale\n",
    "        # Initialize personal best positions and scores\n",
    "        personal_best_positions = particle_positions.copy()\n",
    "        personal_best_scores = [self._evaluate_fitness(p) for p in personal_best_positions]\n",
    "        # Initialize global best position\n",
    "        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Update inertia weight dynamically\n",
    "            w = w_start - (w_start - w_end) * epoch / epochs\n",
    "\n",
    "            for i in range(n_particles):\n",
    "                # Update velocity\n",
    "                r1, r2 = np.random.rand(), np.random.rand()\n",
    "                particle_velocities[i] = (\n",
    "                    w * particle_velocities[i]\n",
    "                    + c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n",
    "                    + c2 * r2 * (global_best_position - particle_positions[i])\n",
    "                )\n",
    "                # Apply velocity clamping if needed\n",
    "                if max_velocity is not None:\n",
    "                    particle_velocities[i] = np.clip(particle_velocities[i], -max_velocity, max_velocity)\n",
    "                # Update position\n",
    "                particle_positions[i] += particle_velocities[i]\n",
    "                # Evaluate fitness\n",
    "                score = self._evaluate_fitness(particle_positions[i])\n",
    "                if score < self._evaluate_fitness(personal_best_positions[i]):\n",
    "                    personal_best_positions[i] = particle_positions[i].copy()\n",
    "                    if score < self._evaluate_fitness(global_best_position):\n",
    "                        global_best_position = particle_positions[i].copy()\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                # Set weights from the global best position and evaluate accuracy\n",
    "                # self._set_weights_from_vector(global_best_position)\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)\n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        # Set final weights from the global best position and evaluate accuracy on test set\n",
    "        self._set_weights_from_vector(global_best_position)\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "    def _latin_hypercube_sampling(self, n_particles):\n",
    "        \"\"\"\n",
    "        Generate Latin Hypercube Samples for particle positions.\n",
    "\n",
    "        Parameters:\n",
    "        - n_particles: Number of particles.\n",
    "\n",
    "        Returns:\n",
    "        - Scaled Latin Hypercube Samples for particle positions.\n",
    "        \"\"\"\n",
    "        samples = lhs(self._get_weights_as_vector().size, samples=n_particles)\n",
    "        min_val = -1.0\n",
    "        max_val = 1.0\n",
    "        scaled_samples = min_val + samples * (max_val - min_val)\n",
    "        return scaled_samples\n",
    "\n",
    "    def _get_weights_as_vector(self):\n",
    "        \"\"\"\n",
    "        Concatenate weights and biases into a single vector.\n",
    "\n",
    "        Returns:\n",
    "        - Flattened vector containing weights and biases.\n",
    "        \"\"\"\n",
    "        return np.concatenate([\n",
    "            self.W1.flatten(), self.b1.flatten(),\n",
    "            self.W2.flatten(), self.b2.flatten(),\n",
    "            self.W3.flatten(), self.b3.flatten()\n",
    "        ])\n",
    "\n",
    "    def _set_weights_from_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Set weights and biases from a vector.\n",
    "\n",
    "        Parameters:\n",
    "        - vector: Vector containing weights and biases.\n",
    "        \"\"\"\n",
    "        sizes = [\n",
    "            self.W1.size, self.b1.size,\n",
    "            self.W2.size, self.b2.size,\n",
    "            self.W3.size, self.b3.size\n",
    "        ]\n",
    "        vectors = np.split(vector, np.cumsum(sizes)[:-1])\n",
    "        self.W1 = vectors[0].reshape(self.W1.shape)\n",
    "        self.b1 = vectors[1].reshape(self.b1.shape)\n",
    "        self.W2 = vectors[2].reshape(self.W2.shape)\n",
    "        self.b2 = vectors[3].reshape(self.b2.shape)\n",
    "        self.W3 = vectors[4].reshape(self.W3.shape)\n",
    "        self.b3 = vectors[5].reshape(self.b3.shape)\n",
    "\n",
    "    def _evaluate_fitness(self, weights_vector):\n",
    "        \"\"\"\n",
    "        Evaluate fitness using weights vector.\n",
    "\n",
    "        Parameters:\n",
    "        - weights_vector: Vector containing weights.\n",
    "\n",
    "        Returns:\n",
    "        - Negative accuracy for minimization.\n",
    "        \"\"\"\n",
    "        self._set_weights_from_vector(weights_vector)\n",
    "        self.forward_prop(X_train)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        return -np.sum(predictions == Y_train) / Y_train.size  # Negative accuracy for minimization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, method, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using the specified method.\n",
    "\n",
    "        Parameters:\n",
    "        - method: String indicating the training method ('GD', 'SGD', 'adam', 'pso').\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        if method == 'GD':\n",
    "            self.train_GD(epochs)\n",
    "        elif method == 'SGD':\n",
    "            self.train_SGD(epochs)\n",
    "        elif method == 'adam':\n",
    "            self.train_adam(epochs)\n",
    "        elif method == 'pso':\n",
    "            self.train_pso(epochs)\n",
    "        else:\n",
    "            raise ValueError('Invalid Training Method')\n",
    "\n",
    "def ReLU(Z):\n",
    "    \"\"\"\n",
    "    ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the ReLU function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the ReLU function.\n",
    "    \"\"\"\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_prime(Z):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the ReLU function.\n",
    "\n",
    "    Returns:\n",
    "    - Derivative of the ReLU function.\n",
    "    \"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the sigmoid function.\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(np.clip(-Z, -4, 4)))\n",
    "    return A\n",
    "\n",
    "def sigmoid_prime(Z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - Derivative of the sigmoid function.\n",
    "    \"\"\"\n",
    "    A = sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return A\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the softmax function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the softmax function.\n",
    "    \"\"\"\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # Shift values for numerical stability\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def one_hot(Y):\n",
    "    \"\"\"\n",
    "    Convert class labels to one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "    - Y: Class labels.\n",
    "\n",
    "    Returns:\n",
    "    - One-hot encoded representation of class labels.\n",
    "    \"\"\"\n",
    "    one_hot_Y = np.zeros((Y.size, 10))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b208f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(784, 32, 16, 10, 0.01, 41000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3950c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 14.3098%\n",
      "Epoch 1: 14.3098%\n",
      "Epoch 2: 14.3098%\n",
      "Epoch 3: 14.3098%\n",
      "Epoch 4: 14.3098%\n",
      "Epoch 5: 14.3098%\n",
      "Epoch 6: 14.3098%\n",
      "Epoch 7: 14.3098%\n",
      "Epoch 8: 14.3098%\n",
      "Epoch 9: 14.3098%\n",
      "Epoch 10: 14.3098%\n",
      "Epoch 11: 14.3098%\n",
      "Epoch 12: 14.3098%\n",
      "Epoch 13: 14.3098%\n",
      "Epoch 14: 14.3098%\n",
      "Epoch 15: 31.3512%\n",
      "Epoch 16: 31.7732%\n",
      "Epoch 17: 32.3341%\n",
      "Epoch 18: 29.378%\n",
      "Epoch 19: 33.4463%\n",
      "Epoch 20: 34.0098%\n",
      "Epoch 21: 35.0073%\n",
      "Epoch 22: 35.1366%\n",
      "Epoch 23: 35.261%\n",
      "Epoch 24: 35.3171%\n",
      "Epoch 25: 35.539%\n",
      "Epoch 26: 35.5829%\n",
      "Epoch 27: 35.6098%\n",
      "Epoch 28: 35.6366%\n",
      "Epoch 29: 35.7122%\n",
      "Epoch 30: 35.6805%\n",
      "Epoch 31: 35.7756%\n",
      "Epoch 32: 35.8098%\n",
      "Epoch 33: 35.7805%\n",
      "Epoch 34: 35.7805%\n",
      "Epoch 35: 35.8439%\n",
      "Epoch 36: 35.8488%\n",
      "Epoch 37: 35.8146%\n",
      "Epoch 38: 35.8146%\n",
      "Epoch 39: 35.8805%\n",
      "Epoch 40: 35.8927%\n",
      "Epoch 41: 35.8537%\n",
      "Epoch 42: 35.8927%\n",
      "Epoch 43: 35.8927%\n",
      "Epoch 44: 35.8951%\n",
      "Epoch 45: 35.8976%\n",
      "Epoch 46: 35.8976%\n",
      "Epoch 47: 35.8976%\n",
      "Epoch 48: 35.8976%\n",
      "Epoch 49: 35.8976%\n",
      "Training complete\n",
      "\n",
      "Test Set Accuracy: 34.6%\n"
     ]
    }
   ],
   "source": [
    "nn.set_learning_rate(0.05)\n",
    "nn.train('pso', 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
