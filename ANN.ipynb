{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import numpy library for numerical computations\n",
    "import pandas as pd  # Import pandas library for data manipulation and analysis\n",
    "from pyDOE import lhs  # Import lhs function from pyDOE module for Latin Hypercube Sampling\n",
    "import matplotlib.pyplot as plt # Import matplotlib to show image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "853a9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MNIST.csv')  # Read the data from the 'MNIST.csv' file into a pandas DataFrame\n",
    "data = np.array(data)  # Convert the DataFrame to a numpy array\n",
    "np.random.shuffle(data)  # Shuffle the rows of the data array randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape  # Get the dimensions of the data array\n",
    "\n",
    "# Extract the first 1000 rows for testing data and transpose them\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]  # Extract the labels for testing data\n",
    "X_test = data_test[1:n]  # Extract the features for testing data\n",
    "X_test = X_test / 255  # Normalise the features by dividing by 255 (maximum pixel value)\n",
    "\n",
    "# Extract the remaining rows for training data and transpose them\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]  # Extract the labels for training data\n",
    "X_train = data_train[1:n]  # Extract the features for training data\n",
    "X_train = X_train / 255  # Normalise the features by dividing by 255 (maximum pixel value)\n",
    "\n",
    "_, m_train = X_train.shape  # Get the number of training examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025dbe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 41000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape  # Check the shape of the training data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d717afea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, Y_train.size)\n",
    "curr_img = X_train[:, idx, None]\n",
    "curr_img = curr_img.reshape((28, 28)) * 255\n",
    "plt.gray()\n",
    "plt.imshow(curr_img)\n",
    "plt.show()\n",
    "print(Y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9031288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size, learning_rate, data_num, bias = 1):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with given parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size: Number of input features.\n",
    "        - hidden_1_size: Number of neurons in the first hidden layer.\n",
    "        - hidden_2_size: Number of neurons in the second hidden layer.\n",
    "        - output_size: Number of output classes.\n",
    "        - learning_rate: Learning rate for gradient descent.\n",
    "        - data_num: Number of data points in the dataset.\n",
    "        - bias: Bias term (default is 1).\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.m = data_num\n",
    "\n",
    "        # Xavier initialization for weights\n",
    "        self.W1 = np.random.randn(hidden_1_size, input_size) * np.sqrt(1.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_1_size, bias))\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_2_size, hidden_1_size) * np.sqrt(1.0 / hidden_1_size)\n",
    "        self.b2 = np.zeros((hidden_2_size, bias))\n",
    "\n",
    "        self.W3 = np.random.randn(output_size, hidden_2_size) * np.sqrt(1.0 / hidden_2_size)\n",
    "        self.b3 = np.zeros((output_size, bias))\n",
    "\n",
    "    def set_learning_rate(self, new_LR):\n",
    "        \"\"\"\n",
    "        Update the learning rate.\n",
    "\n",
    "        Parameters:\n",
    "        - new_LR: New learning rate value.\n",
    "        \"\"\"\n",
    "        self.learning_rate = new_LR\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data matrix.\n",
    "\n",
    "        Returns:\n",
    "        - A3: Output predictions after the final activation function.\n",
    "        - Z3: Output of the final layer before the activation function.\n",
    "        \"\"\"\n",
    "        self.Z1 = self.W1.dot(X) + self.b1\n",
    "        self.A1 = ReLU(self.Z1)\n",
    "\n",
    "        self.Z2 = self.W2.dot(self.A1) + self.b2\n",
    "        self.A2 = ReLU(self.Z2)\n",
    "\n",
    "        self.Z3 = self.W3.dot(self.A2) + self.b3\n",
    "        self.A3 = softmax(self.Z3)\n",
    "\n",
    "        return self.A3, self.Z3\n",
    "\n",
    "    def backward_prop(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform backward propagation to compute gradients.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data matrix.\n",
    "        - y: True labels.\n",
    "        \"\"\"\n",
    "        one_hot_Y = one_hot(y)\n",
    "\n",
    "        self.dZ3 = self.A3 - one_hot_Y\n",
    "        self.dW3 = 1 / self.m * self.dZ3.dot(self.A2.T)\n",
    "        self.db3 = 1 / self.m * np.sum(self.dZ3, axis=1, keepdims=True)\n",
    "\n",
    "        self.dZ2 = self.W3.T.dot(self.dZ3) * ReLU_prime(self.Z2)\n",
    "        self.dW2 = 1 / self.m * self.dZ2.dot(self.A1.T)\n",
    "        self.db2 = 1 / self.m * np.sum(self.dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        self.dZ1 = self.W2.T.dot(self.dZ2) * ReLU_prime(self.Z1)\n",
    "        self.dW1 = 1 / self.m * self.dZ1.dot(X.T)\n",
    "        self.db1 = 1 / self.m * np.sum(self.dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        \"\"\"\n",
    "        Update weights using gradient descent.\n",
    "        \"\"\"\n",
    "        self.W1 = self.W1 - self.dW1 * self.learning_rate\n",
    "        self.b1 = self.b1 - self.db1 * self.learning_rate\n",
    "\n",
    "        self.W2 = self.W2 - self.dW2 * self.learning_rate\n",
    "        self.b2 = self.b2 - self.db2 * self.learning_rate\n",
    "\n",
    "        self.W3 = self.W3 - self.dW3 * self.learning_rate\n",
    "        self.b3 = self.b3 - self.db3 * self.learning_rate\n",
    "\n",
    "    def train_GD(self, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using batch gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "            self.gradient_descent()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "\n",
    "    def train_SGD(self, epochs, batch = 64):\n",
    "        num_samples = X_train.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indicies = np.random.permutation(num_samples)\n",
    "            X_train_shuffled = X_train[:, indicies]\n",
    "            Y_train_shuffled = Y_train[indicies]\n",
    "\n",
    "            for i in range(0, num_samples, batch):\n",
    "                X_batch = X_train_shuffled[:, i:i+batch]\n",
    "                Y_batch = Y_train_shuffled[i:i+batch]\n",
    "\n",
    "                self.forward_prop(X_batch)\n",
    "                self.backward_prop(X_batch, Y_batch)\n",
    "                self.gradient_descent()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "    def train_adam(self, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using the Adam optimization algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"        \n",
    "        #Init Extra Weights\n",
    "        self.beta1 = 0.6 # Exp decay rate for mean of gradients\n",
    "        self.beta2 = 0.6 # Exp decay rate for varience of gradients\n",
    "        self.epsilon = 1e-8 # Prevent divisions by 0\n",
    "        self.m_W1 = np.zeros_like(self.W1) # Moving average of gradients\n",
    "        self.v_W1 = np.zeros_like(self.W1) # Squared moving averages of gradients\n",
    "        self.m_b1 = np.zeros_like(self.b1)\n",
    "        self.v_b1 = np.zeros_like(self.b1)\n",
    "        self.m_W2 = np.zeros_like(self.W2)\n",
    "        self.v_W2 = np.zeros_like(self.W2)\n",
    "        self.m_b2 = np.zeros_like(self.b2)\n",
    "        self.v_b2 = np.zeros_like(self.b2)\n",
    "        self.m_W3 = np.zeros_like(self.W3)\n",
    "        self.v_W3 = np.zeros_like(self.W3)\n",
    "        self.m_b3 = np.zeros_like(self.b3)\n",
    "        self.v_b3 = np.zeros_like(self.b3)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Perform forward and backward propagation\n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "\n",
    "            # Update parameters using Adam optimization rule\n",
    "            self.adam_update()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                # Evaluate and print accuracy every 10 epochs\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)\n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        # Evaluate accuracy on test set after training\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "    def adam_update(self):\n",
    "        \"\"\"\n",
    "        Update weights and biases using the Adam optimization algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update the first moment estimate of gradients for weights W1 using exponential decay\n",
    "        self.m_W1 = self.beta1 * self.m_W1 + (1 - self.beta1) * self.dW1\n",
    "\n",
    "        # Update the second moment estimate of gradients for weights W1 using exponential decay\n",
    "        self.v_W1 = self.beta2 * self.v_W1 + (1 - self.beta2) * (self.dW1 ** 2)\n",
    "\n",
    "        # Correct bias in the first moment estimate\n",
    "        m_W1_hat = self.m_W1 / (1 - self.beta1)\n",
    "\n",
    "        # Correct bias in the second moment estimate\n",
    "        v_W1_hat = self.v_W1 / (1 - self.beta2)\n",
    "\n",
    "        # Update weights W1 using the Adam optimization update rule\n",
    "        self.W1 -= self.learning_rate * m_W1_hat / (np.sqrt(v_W1_hat) + self.epsilon)\n",
    "\n",
    "        #Repeat for following weights and biases\n",
    "\n",
    "        self.m_b1 = self.beta1 * self.m_b1 + (1 - self.beta1) * self.db1\n",
    "        self.v_b1 = self.beta2 * self.v_b1 + (1 - self.beta2) * (self.db1 ** 2)\n",
    "        m_b1_hat = self.m_b1 / (1 - self.beta1)\n",
    "        v_b1_hat = self.v_b1 / (1 - self.beta2)\n",
    "        self.b1 -= self.learning_rate * m_b1_hat / (np.sqrt(v_b1_hat) + self.epsilon)\n",
    "\n",
    "        self.m_W2 = self.beta1 * self.m_W2 + (1 - self.beta1) * self.dW2\n",
    "        self.v_W2 = self.beta2 * self.v_W2 + (1 - self.beta2) * (self.dW2 ** 2)\n",
    "        m_W2_hat = self.m_W2 / (1 - self.beta1)\n",
    "        v_W2_hat = self.v_W2 / (1 - self.beta2)\n",
    "        self.W2 -= self.learning_rate * m_W2_hat / (np.sqrt(v_W2_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b2 = self.beta1 * self.m_b2 + (1 - self.beta1) * self.db2\n",
    "        self.v_b2 = self.beta2 * self.v_b2 + (1 - self.beta2) * (self.db2 ** 2)\n",
    "        m_b2_hat = self.m_b2 / (1 - self.beta1)\n",
    "        v_b2_hat = self.v_b2 / (1 - self.beta2)\n",
    "        self.b2 -= self.learning_rate * m_b2_hat / (np.sqrt(v_b2_hat) + self.epsilon)\n",
    "\n",
    "        self.m_W3 = self.beta1 * self.m_W3 + (1 - self.beta1) * self.dW3\n",
    "        self.v_W3 = self.beta2 * self.v_W3 + (1 - self.beta2) * (self.dW3 ** 2)\n",
    "        m_W3_hat = self.m_W3 / (1 - self.beta1)\n",
    "        v_W3_hat = self.v_W3 / (1 - self.beta2)\n",
    "        self.W3 -= self.learning_rate * m_W3_hat / (np.sqrt(v_W3_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b3 = self.beta1 * self.m_b3 + (1 - self.beta1) * self.db3\n",
    "        self.v_b3 = self.beta2 * self.v_b3 + (1 - self.beta2) * (self.db3 ** 2)\n",
    "        m_b3_hat = self.m_b3 / (1 - self.beta1)\n",
    "        v_b3_hat = self.v_b3 / (1 - self.beta2)\n",
    "        self.b3 -= self.learning_rate * m_b3_hat / (np.sqrt(v_b3_hat) + self.epsilon)\n",
    "\n",
    "    \n",
    "   \n",
    "    def train_pso(self, epochs, n_particles=100, w_start=0.9, w_end=0.3, c1=0.5, c2=0.3, max_velocity=None):\n",
    "        \"\"\"\n",
    "        Train the neural network using Particle Swarm Optimization (PSO).\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        - n_particles: Number of particles (default is 50).\n",
    "        - w_start: Initial inertia weight (default is 0.9).\n",
    "        - w_end: Final inertia weight (default is 0.2).\n",
    "        - c1: Cognitive parameter (default is 1.5).\n",
    "        - c2: Social parameter (default is 1.5).\n",
    "        - max_velocity: Maximum velocity for particle movement (default is None).\n",
    "        \"\"\"\n",
    "        # Initialize particles' positions using Latin Hypercube Sampling\n",
    "        particle_positions = self._latin_hypercube_sampling(n_particles)\n",
    "        # Initialize particles' velocities\n",
    "        particle_velocities = [np.random.randn(len(particle_positions[0])) * 0.1 for _ in range(n_particles)]  # Adjust the scale\n",
    "        # Initialize personal best positions and scores\n",
    "        personal_best_positions = particle_positions.copy()\n",
    "        personal_best_scores = [self._evaluate_fitness(p) for p in personal_best_positions]\n",
    "        # Initialize global best position\n",
    "        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Update inertia weight dynamically\n",
    "            w = w_start - (w_start - w_end) * epoch / epochs\n",
    "\n",
    "            for i in range(n_particles):\n",
    "                # Update velocity\n",
    "                r1, r2 = np.random.rand(), np.random.rand()\n",
    "                particle_velocities[i] = (\n",
    "                    w * particle_velocities[i]\n",
    "                    + c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n",
    "                    + c2 * r2 * (global_best_position - particle_positions[i])\n",
    "                )\n",
    "                # Apply velocity clamping if needed\n",
    "                if max_velocity is not None:\n",
    "                    particle_velocities[i] = np.clip(particle_velocities[i], -max_velocity, max_velocity)\n",
    "                # Update position\n",
    "                particle_positions[i] += particle_velocities[i]\n",
    "                # Evaluate fitness\n",
    "                score = self._evaluate_fitness(particle_positions[i])\n",
    "                if score < self._evaluate_fitness(personal_best_positions[i]):\n",
    "                    personal_best_positions[i] = particle_positions[i].copy()\n",
    "                    if score < self._evaluate_fitness(global_best_position):\n",
    "                        global_best_position = particle_positions[i].copy()\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                # Set weights from the global best position and evaluate accuracy\n",
    "                # self._set_weights_from_vector(global_best_position)\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)\n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        # Set final weights from the global best position and evaluate accuracy on test set\n",
    "        self._set_weights_from_vector(global_best_position)\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "    def _latin_hypercube_sampling(self, n_particles):\n",
    "        \"\"\"\n",
    "        Generate Latin Hypercube Samples for particle positions.\n",
    "\n",
    "        Parameters:\n",
    "        - n_particles: Number of particles.\n",
    "\n",
    "        Returns:\n",
    "        - Scaled Latin Hypercube Samples for particle positions.\n",
    "        \"\"\"\n",
    "        samples = lhs(self._get_weights_as_vector().size, samples=n_particles)\n",
    "        min_val = -1.0\n",
    "        max_val = 1.0\n",
    "        scaled_samples = min_val + samples * (max_val - min_val)\n",
    "        return scaled_samples\n",
    "\n",
    "    def _get_weights_as_vector(self):\n",
    "        \"\"\"\n",
    "        Concatenate weights and biases into a single vector.\n",
    "\n",
    "        Returns:\n",
    "        - Flattened vector containing weights and biases.\n",
    "        \"\"\"\n",
    "        return np.concatenate([\n",
    "            self.W1.flatten(), self.b1.flatten(),\n",
    "            self.W2.flatten(), self.b2.flatten(),\n",
    "            self.W3.flatten(), self.b3.flatten()\n",
    "        ])\n",
    "\n",
    "    def _set_weights_from_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Set weights and biases from a vector.\n",
    "\n",
    "        Parameters:\n",
    "        - vector: Vector containing weights and biases.\n",
    "        \"\"\"\n",
    "        sizes = [\n",
    "            self.W1.size, self.b1.size,\n",
    "            self.W2.size, self.b2.size,\n",
    "            self.W3.size, self.b3.size\n",
    "        ]\n",
    "        vectors = np.split(vector, np.cumsum(sizes)[:-1])\n",
    "        self.W1 = vectors[0].reshape(self.W1.shape)\n",
    "        self.b1 = vectors[1].reshape(self.b1.shape)\n",
    "        self.W2 = vectors[2].reshape(self.W2.shape)\n",
    "        self.b2 = vectors[3].reshape(self.b2.shape)\n",
    "        self.W3 = vectors[4].reshape(self.W3.shape)\n",
    "        self.b3 = vectors[5].reshape(self.b3.shape)\n",
    "\n",
    "    def _evaluate_fitness(self, weights_vector):\n",
    "        \"\"\"\n",
    "        Evaluate fitness using weights vector.\n",
    "\n",
    "        Parameters:\n",
    "        - weights_vector: Vector containing weights.\n",
    "\n",
    "        Returns:\n",
    "        - Negative accuracy for minimization.\n",
    "        \"\"\"\n",
    "        self._set_weights_from_vector(weights_vector)\n",
    "        self.forward_prop(X_train)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        return -np.sum(predictions == Y_train) / Y_train.size  # Negative accuracy for minimization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, method, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using the specified method.\n",
    "\n",
    "        Parameters:\n",
    "        - method: String indicating the training method ('GD', 'SGD', 'adam', 'pso').\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        if method == 'GD':\n",
    "            self.train_GD(epochs)\n",
    "        elif method == 'SGD':\n",
    "            self.train_SGD(epochs)\n",
    "        elif method == 'adam':\n",
    "            self.train_adam(epochs)\n",
    "        elif method == 'pso':\n",
    "            self.train_pso(epochs)\n",
    "        else:\n",
    "            raise ValueError('Invalid Training Method')\n",
    "\n",
    "def ReLU(Z):\n",
    "    \"\"\"\n",
    "    ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the ReLU function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the ReLU function.\n",
    "    \"\"\"\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_prime(Z):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the ReLU function.\n",
    "\n",
    "    Returns:\n",
    "    - Derivative of the ReLU function.\n",
    "    \"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the sigmoid function.\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(np.clip(-Z, -4, 4)))\n",
    "    return A\n",
    "\n",
    "def sigmoid_prime(Z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - Derivative of the sigmoid function.\n",
    "    \"\"\"\n",
    "    A = sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return A\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the softmax function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the softmax function.\n",
    "    \"\"\"\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # Shift values for numerical stability\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def one_hot(Y):\n",
    "    \"\"\"\n",
    "    Convert class labels to one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "    - Y: Class labels.\n",
    "\n",
    "    Returns:\n",
    "    - One-hot encoded representation of class labels.\n",
    "    \"\"\"\n",
    "    one_hot_Y = np.zeros((Y.size, 10))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b208f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(784, 32, 16, 10, 0.01, 41000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3950c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.set_learning_rate(0.05)\n",
    "nn.train('adam', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d2bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
