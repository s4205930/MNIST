{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "7.5 %\n",
      "1\n",
      "7.1 %\n",
      "2\n",
      "7.2 %\n",
      "3\n",
      "7.7 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m new_population \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pop_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m--> 115\u001b[0m     parent_1, parent_2 \u001b[38;5;241m=\u001b[39m \u001b[43mselect_parents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     child_1 \u001b[38;5;241m=\u001b[39m crossover(parent_1, parent_2)\n\u001b[0;32m    118\u001b[0m     child_1 \u001b[38;5;241m=\u001b[39m mutate(child_1, mutation_rate)\n",
      "Cell \u001b[1;32mIn[2], line 82\u001b[0m, in \u001b[0;36mselect_parents\u001b[1;34m(population, X, y)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_parents\u001b[39m(population, X, y):\n\u001b[1;32m---> 82\u001b[0m     fitness_scores \u001b[38;5;241m=\u001b[39m [\u001b[43mfitness_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m network \u001b[38;5;129;01min\u001b[39;00m population]\n\u001b[0;32m     83\u001b[0m     total_fitness \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(fitness_scores)\n\u001b[0;32m     84\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m [score \u001b[38;5;241m/\u001b[39m total_fitness \u001b[38;5;28;01mfor\u001b[39;00m score \u001b[38;5;129;01min\u001b[39;00m fitness_scores]\n",
      "Cell \u001b[1;32mIn[2], line 64\u001b[0m, in \u001b[0;36mfitness_function\u001b[1;34m(network, X, y)\u001b[0m\n\u001b[0;32m     61\u001b[0m nn\u001b[38;5;241m.\u001b[39mW2 \u001b[38;5;241m=\u001b[39m network[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     62\u001b[0m nn\u001b[38;5;241m.\u001b[39mb2 \u001b[38;5;241m=\u001b[39m network[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 64\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n",
      "Cell \u001b[1;32mIn[2], line 45\u001b[0m, in \u001b[0;36mNeural_Network.forward_prop\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_prop\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m     44\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW1\u001b[38;5;241m.\u001b[39mdot(X) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1\n\u001b[1;32m---> 45\u001b[0m     A1 \u001b[38;5;241m=\u001b[39m \u001b[43mReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     Z2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2\u001b[38;5;241m.\u001b[39mdot(A1)\n\u001b[0;32m     48\u001b[0m     A2 \u001b[38;5;241m=\u001b[39m softmax(Z2)\n",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m, in \u001b[0;36mReLU\u001b[1;34m(Z)\u001b[0m\n\u001b[0;32m     18\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m     20\u001b[0m _, m_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mReLU\u001b[39m(Z):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmaximum(Z, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(Z):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('MNIST.csv')\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "m, n = data.shape\n",
    "\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]\n",
    "X_test = data_test[1:n]\n",
    "X_test = X_test / 255\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255\n",
    "\n",
    "_, m_train = X_train.shape\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "class Neural_Network:\n",
    "    def __init__(self, input_size, hidden_1_size, output_size, bias = 1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.W1 = np.random.rand(hidden_1_size, input_size) - 0.5\n",
    "        self.b1 = np.random.rand(hidden_1_size, bias) - 0.5\n",
    "\n",
    "        self.W2 = np.random.rand(output_size, hidden_1_size) - 0.5\n",
    "        self.b2 = np.random.rand(output_size, bias) - 0.5\n",
    "\n",
    "\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        Z1 = self.W1.dot(X) + self.b1\n",
    "        A1 = ReLU(Z1)\n",
    "\n",
    "        Z2 = self.W2.dot(A1)\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        return A2\n",
    "\n",
    "\n",
    "pop_size = 50\n",
    "mutation_rate = 0.1\n",
    "generations = 10\n",
    "\n",
    "def fitness_function(network, X, y):\n",
    "    nn = Neural_Network(input_size=784, hidden_1_size=10, output_size=10)\n",
    "    nn.W1 = network['W1']\n",
    "    nn.b1 = network['b1']\n",
    "    nn.W2 = network['W2']\n",
    "    nn.b2 = network['b2']\n",
    "\n",
    "    predictions = nn.forward_prop(X)\n",
    "    accuracy = np.mean(np.argmax(predictions, axis=0) == y)\n",
    "    return accuracy\n",
    "\n",
    "def init_pop(nn, pop_size):\n",
    "    population = []\n",
    "\n",
    "    for _ in range(pop_size):\n",
    "        network = {\n",
    "            'W1': nn.W1.copy(),\n",
    "            'b1': nn.b1.copy(),\n",
    "            'W2': nn.W2.copy(),\n",
    "            'b2': nn.b2.copy(),\n",
    "        }\n",
    "        population.append(network)\n",
    "    return population\n",
    "\n",
    "def select_parents(population, X, y):\n",
    "    fitness_scores = [fitness_function(network, X, y) for network in population]\n",
    "    total_fitness = sum(fitness_scores)\n",
    "    probabilities = [score / total_fitness for score in fitness_scores]\n",
    "    selected_indices = np.random.choice(len(population), size = 2, p = probabilities, replace = False)\n",
    "    parent_1 = population[selected_indices[0]].copy()\n",
    "    parent_2 = population[selected_indices[1]].copy()\n",
    "    return parent_1, parent_2\n",
    "\n",
    "def crossover(parent_1, parent_2):\n",
    "    child = {}\n",
    "    for key in parent_1:\n",
    "        if np.random.rand() < 0.5:\n",
    "            child[key] = parent_1[key]\n",
    "        else:\n",
    "            child[key] = parent_2[key]\n",
    "    return child\n",
    "\n",
    "def mutate(network, mutation_rate):\n",
    "    for key in network:\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            network[key] += np.random.randn(*network[key].shape) * 0.1\n",
    "    return network\n",
    "\n",
    "nn = Neural_Network(784, 10, 10)\n",
    "population = init_pop(nn, pop_size)\n",
    "best_network = population[0]\n",
    "\n",
    "for generation in range(generations):\n",
    "    print(generation)\n",
    "    print(round(fitness_function(best_network, X_test, Y_test) * 100, 4), '%')\n",
    "    new_population = []\n",
    "\n",
    "    for _ in range(pop_size // 2):\n",
    "        parent_1, parent_2 = select_parents(population, X_train, Y_train)\n",
    "\n",
    "        child_1 = crossover(parent_1, parent_2)\n",
    "        child_1 = mutate(child_1, mutation_rate)\n",
    "\n",
    "        child_2 = crossover(parent_1, parent_2)\n",
    "        child_2 = mutate(child_2, mutation_rate)\n",
    "\n",
    "        new_population.extend([child_1, child_2])\n",
    "    population = new_population \n",
    "\n",
    "best_network = max(population, key=lambda x: fitness_function(x, X_train, Y_train))\n",
    "print(round(fitness_function(best_network, X_test, Y_test) * 100, 4), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
