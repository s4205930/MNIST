{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b46e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import numpy library for numerical computations\n",
    "import pandas as pd  # Import pandas library for data manipulation and analysis\n",
    "from pyDOE import lhs  # Import lhs function from pyDOE module for Latin Hypercube Sampling\n",
    "import matplotlib.pyplot as plt # Import matplotlib to show image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "853a9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MNIST.csv')  # Read the data from the 'MNIST.csv' file into a pandas DataFrame\n",
    "data = np.array(data)  # Convert the DataFrame to a numpy array\n",
    "np.random.shuffle(data)  # Shuffle the rows of the data array randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f95c18dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape  # Get the dimensions of the data array\n",
    "\n",
    "# Extract the first 1000 rows for testing data and transpose them\n",
    "data_test = data[0:1000].T\n",
    "Y_test = data_test[0]  # Extract the labels for testing data\n",
    "X_test = data_test[1:n]  # Extract the features for testing data\n",
    "X_test = X_test / 255  # Normalise the features by dividing by 255 (maximum pixel value)\n",
    "\n",
    "# Extract the remaining rows for training data and transpose them\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]  # Extract the labels for training data\n",
    "X_train = data_train[1:n]  # Extract the features for training data\n",
    "X_train = X_train / 255  # Normalise the features by dividing by 255 (maximum pixel value)\n",
    "\n",
    "_, m_train = X_train.shape  # Get the number of training examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "025dbe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 41000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape  # Check the shape of the training data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d717afea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, Y_train.size)\n",
    "curr_img = X_train[:, idx, None]\n",
    "curr_img = curr_img.reshape((28, 28)) * 255\n",
    "plt.gray()\n",
    "plt.imshow(curr_img)\n",
    "plt.show()\n",
    "print(Y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9031288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size, learning_rate, data_num, bias = 1):\n",
    "        \"\"\"\n",
    "        initialise the neural network with given parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - input_size: Number of input features.\n",
    "        - hidden_1_size: Number of neurons in the first hidden layer.\n",
    "        - hidden_2_size: Number of neurons in the second hidden layer.\n",
    "        - output_size: Number of output classes.\n",
    "        - learning_rate: Learning rate for gradient descent.\n",
    "        - data_num: Number of data points in the dataset.\n",
    "        - bias: Bias term (default is 1).\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_1_size = hidden_1_size\n",
    "        self.hidden_2_size = hidden_2_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.m = data_num\n",
    "\n",
    "        # Xavier initialisation for weights\n",
    "        self.W1 = np.random.randn(hidden_1_size, input_size) * np.sqrt(1.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_1_size, bias))\n",
    "\n",
    "        self.W2 = np.random.randn(hidden_2_size, hidden_1_size) * np.sqrt(1.0 / hidden_1_size)\n",
    "        self.b2 = np.zeros((hidden_2_size, bias))\n",
    "\n",
    "        self.W3 = np.random.randn(output_size, hidden_2_size) * np.sqrt(1.0 / hidden_2_size)\n",
    "        self.b3 = np.zeros((output_size, bias))\n",
    "\n",
    "    def set_learning_rate(self, new_LR):\n",
    "        \"\"\"\n",
    "        Update the learning rate.\n",
    "\n",
    "        Parameters:\n",
    "        - new_LR: New learning rate value.\n",
    "        \"\"\"\n",
    "        self.learning_rate = new_LR\n",
    "\n",
    "    def forward_prop(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data matrix.\n",
    "        \"\"\"\n",
    "        self.Z1 = self.W1.dot(X) + self.b1\n",
    "        self.A1 = ReLU(self.Z1)\n",
    "\n",
    "        self.Z2 = self.W2.dot(self.A1) + self.b2\n",
    "        self.A2 = ReLU(self.Z2)\n",
    "\n",
    "        self.Z3 = self.W3.dot(self.A2) + self.b3\n",
    "        self.A3 = softmax(self.Z3)\n",
    "\n",
    "\n",
    "    def backward_prop(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform backward propagation to compute gradients.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data matrix.\n",
    "        - y: True labels.\n",
    "        \"\"\"\n",
    "        one_hot_Y = one_hot(y)\n",
    "\n",
    "        self.dZ3 = self.A3 - one_hot_Y\n",
    "        self.dW3 = 1 / self.m * self.dZ3.dot(self.A2.T)\n",
    "        self.db3 = 1 / self.m * np.sum(self.dZ3, axis=1, keepdims=True)\n",
    "\n",
    "        self.dZ2 = self.W3.T.dot(self.dZ3) * ReLU_prime(self.Z2)\n",
    "        self.dW2 = 1 / self.m * self.dZ2.dot(self.A1.T)\n",
    "        self.db2 = 1 / self.m * np.sum(self.dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        self.dZ1 = self.W2.T.dot(self.dZ2) * ReLU_prime(self.Z1)\n",
    "        self.dW1 = 1 / self.m * self.dZ1.dot(X.T)\n",
    "        self.db1 = 1 / self.m * np.sum(self.dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        \"\"\"\n",
    "        Update weights using gradient descent.\n",
    "        \"\"\"\n",
    "        self.W1 = self.W1 - self.dW1 * self.learning_rate\n",
    "        self.b1 = self.b1 - self.db1 * self.learning_rate\n",
    "\n",
    "        self.W2 = self.W2 - self.dW2 * self.learning_rate\n",
    "        self.b2 = self.b2 - self.db2 * self.learning_rate\n",
    "\n",
    "        self.W3 = self.W3 - self.dW3 * self.learning_rate\n",
    "        self.b3 = self.b3 - self.db3 * self.learning_rate\n",
    "\n",
    "    def train_GD(self, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using batch gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "            self.gradient_descent()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "\n",
    "    def train_SGD(self, epochs, batch = 64):\n",
    "        num_samples = X_train.shape[1]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indicies = np.random.permutation(num_samples)\n",
    "            X_train_shuffled = X_train[:, indicies]\n",
    "            Y_train_shuffled = Y_train[indicies]\n",
    "\n",
    "            for i in range(0, num_samples, batch):\n",
    "                X_batch = X_train_shuffled[:, i:i+batch]\n",
    "                Y_batch = Y_train_shuffled[i:i+batch]\n",
    "\n",
    "                self.forward_prop(X_batch)\n",
    "                self.backward_prop(X_batch, Y_batch)\n",
    "                self.gradient_descent()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)        \n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "    def train_adam(self, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using the Adam optimization algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"        \n",
    "        #Init Extra Weights\n",
    "        self.beta1 = 0.6 # Exp decay rate for mean of gradients\n",
    "        self.beta2 = 0.6 # Exp decay rate for varience of gradients\n",
    "        self.epsilon = 1e-8 # Prevent divisions by 0\n",
    "        \n",
    "        self.m_W1 = np.zeros_like(self.W1) # Moving average of gradients\n",
    "        self.v_W1 = np.zeros_like(self.W1) # Squared moving averages of gradients\n",
    "        self.m_b1 = np.zeros_like(self.b1)\n",
    "        self.v_b1 = np.zeros_like(self.b1)\n",
    "        self.m_W2 = np.zeros_like(self.W2)\n",
    "        self.v_W2 = np.zeros_like(self.W2)\n",
    "        self.m_b2 = np.zeros_like(self.b2)\n",
    "        self.v_b2 = np.zeros_like(self.b2)\n",
    "        self.m_W3 = np.zeros_like(self.W3)\n",
    "        self.v_W3 = np.zeros_like(self.W3)\n",
    "        self.m_b3 = np.zeros_like(self.b3)\n",
    "        self.v_b3 = np.zeros_like(self.b3)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Perform forward and backward propagation\n",
    "            self.forward_prop(X_train)\n",
    "            self.backward_prop(X_train, Y_train)\n",
    "\n",
    "            # Update parameters using Adam optimization rule\n",
    "            self.adam_update()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                # Evaluate and print accuracy every 10 epochs\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)\n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        # Evaluate accuracy on test set after training\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "    def adam_update(self):\n",
    "        \"\"\"\n",
    "        Update weights and biases using the Adam optimization algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update the first moment estimate of gradients for weights W1 using exponential decay\n",
    "        self.m_W1 = self.beta1 * self.m_W1 + (1 - self.beta1) * self.dW1\n",
    "\n",
    "        # Update the second moment estimate of gradients for weights W1 using exponential decay\n",
    "        self.v_W1 = self.beta2 * self.v_W1 + (1 - self.beta2) * (self.dW1 ** 2)\n",
    "\n",
    "        # Correct bias in the first moment estimate\n",
    "        m_W1_hat = self.m_W1 / (1 - self.beta1)\n",
    "\n",
    "        # Correct bias in the second moment estimate\n",
    "        v_W1_hat = self.v_W1 / (1 - self.beta2)\n",
    "\n",
    "        # Update weights W1 using the Adam optimisation update rule\n",
    "        self.W1 -= self.learning_rate * m_W1_hat / (np.sqrt(v_W1_hat) + self.epsilon)\n",
    "\n",
    "        #Repeat for following weights and biases\n",
    "\n",
    "        self.m_b1 = self.beta1 * self.m_b1 + (1 - self.beta1) * self.db1\n",
    "        self.v_b1 = self.beta2 * self.v_b1 + (1 - self.beta2) * (self.db1 ** 2)\n",
    "        m_b1_hat = self.m_b1 / (1 - self.beta1)\n",
    "        v_b1_hat = self.v_b1 / (1 - self.beta2)\n",
    "        self.b1 -= self.learning_rate * m_b1_hat / (np.sqrt(v_b1_hat) + self.epsilon)\n",
    "\n",
    "        self.m_W2 = self.beta1 * self.m_W2 + (1 - self.beta1) * self.dW2\n",
    "        self.v_W2 = self.beta2 * self.v_W2 + (1 - self.beta2) * (self.dW2 ** 2)\n",
    "        m_W2_hat = self.m_W2 / (1 - self.beta1)\n",
    "        v_W2_hat = self.v_W2 / (1 - self.beta2)\n",
    "        self.W2 -= self.learning_rate * m_W2_hat / (np.sqrt(v_W2_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b2 = self.beta1 * self.m_b2 + (1 - self.beta1) * self.db2\n",
    "        self.v_b2 = self.beta2 * self.v_b2 + (1 - self.beta2) * (self.db2 ** 2)\n",
    "        m_b2_hat = self.m_b2 / (1 - self.beta1)\n",
    "        v_b2_hat = self.v_b2 / (1 - self.beta2)\n",
    "        self.b2 -= self.learning_rate * m_b2_hat / (np.sqrt(v_b2_hat) + self.epsilon)\n",
    "\n",
    "        self.m_W3 = self.beta1 * self.m_W3 + (1 - self.beta1) * self.dW3\n",
    "        self.v_W3 = self.beta2 * self.v_W3 + (1 - self.beta2) * (self.dW3 ** 2)\n",
    "        m_W3_hat = self.m_W3 / (1 - self.beta1)\n",
    "        v_W3_hat = self.v_W3 / (1 - self.beta2)\n",
    "        self.W3 -= self.learning_rate * m_W3_hat / (np.sqrt(v_W3_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b3 = self.beta1 * self.m_b3 + (1 - self.beta1) * self.db3\n",
    "        self.v_b3 = self.beta2 * self.v_b3 + (1 - self.beta2) * (self.db3 ** 2)\n",
    "        m_b3_hat = self.m_b3 / (1 - self.beta1)\n",
    "        v_b3_hat = self.v_b3 / (1 - self.beta2)\n",
    "        self.b3 -= self.learning_rate * m_b3_hat / (np.sqrt(v_b3_hat) + self.epsilon)\n",
    "\n",
    "    \n",
    "   \n",
    "    def train_pso(self, epochs, n_particles=50, w_start=0.9, w_end=0.3, c1=0.5, c2=0.3):\n",
    "        \"\"\"\n",
    "        Train the neural network using Particle Swarm Optimization (PSO).\n",
    "\n",
    "        Parameters:\n",
    "        - epochs: Number of training epochs.\n",
    "        - n_particles: Number of particles (default is 50).\n",
    "        - w_start: Initial inertia weight (default is 0.9).\n",
    "        - w_end: Final inertia weight (default is 0.2).\n",
    "        - c1: Cognitive parameter (default is 1.5).\n",
    "        - c2: Social parameter (default is 1.5).\n",
    "        \"\"\"\n",
    "        # initialise particles' positions using Latin Hypercube Sampling\n",
    "        particle_positions = self._latin_hypercube_sampling(n_particles)\n",
    "        # initialise particles' velocities\n",
    "        particle_velocities = [np.random.randn(len(particle_positions[0])) * 0.1 for _ in range(n_particles)]\n",
    "        # initialise personal best positions and scores\n",
    "        personal_best_positions = particle_positions.copy()\n",
    "        personal_best_scores = [self._evaluate_fitness(p) for p in personal_best_positions]\n",
    "        # initialise global best position\n",
    "        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Update inertia weight dynamically\n",
    "            w = w_start - (w_start - w_end) * epoch / epochs\n",
    "\n",
    "            for i in range(n_particles):\n",
    "                # Update velocity\n",
    "                r1, r2 = np.random.rand(), np.random.rand()\n",
    "                particle_velocities[i] = (\n",
    "                    w * particle_velocities[i]\n",
    "                    + c1 * r1 * (personal_best_positions[i] - particle_positions[i])\n",
    "                    + c2 * r2 * (global_best_position - particle_positions[i])\n",
    "                )\n",
    "                \n",
    "                # Update position\n",
    "                particle_positions[i] += particle_velocities[i]\n",
    "                # Evaluate fitness\n",
    "                score = self._evaluate_fitness(particle_positions[i])\n",
    "                if score < self._evaluate_fitness(personal_best_positions[i]):\n",
    "                    personal_best_positions[i] = particle_positions[i].copy()\n",
    "                    if score < self._evaluate_fitness(global_best_position):\n",
    "                        global_best_position = particle_positions[i].copy()\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                # Set weights from the global best position and evaluate accuracy\n",
    "                self._set_weights_from_vector(global_best_position)\n",
    "                self.forward_prop(X_train)\n",
    "                predictions = np.argmax(self.A3, axis=0)\n",
    "                print(f'Epoch {epoch}: {round((np.sum(predictions == Y_train) / Y_train.size) * 100, 4)}%')\n",
    "\n",
    "        # Set final weights from the global best position and evaluate accuracy on test set\n",
    "        self._set_weights_from_vector(global_best_position)\n",
    "        self.forward_prop(X_test)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        print(\"Training complete\\n\")\n",
    "        print(f'Test Set Accuracy: {round((np.sum(predictions == Y_test) / Y_test.size) * 100, 4)}%')\n",
    "\n",
    "\n",
    "    def _latin_hypercube_sampling(self, n_particles):\n",
    "        \"\"\"\n",
    "        Generate Latin Hypercube Samples for particle positions.\n",
    "\n",
    "        Parameters:\n",
    "        - n_particles: Number of particles.\n",
    "\n",
    "        Returns:\n",
    "        - Scaled Latin Hypercube Samples for particle positions.\n",
    "        \"\"\"\n",
    "        samples = lhs(self._get_weights_as_vector().size, samples=n_particles)\n",
    "        min_val = -1.0\n",
    "        max_val = 1.0\n",
    "        scaled_samples = min_val + samples * (max_val - min_val)\n",
    "        return scaled_samples\n",
    "\n",
    "    def _get_weights_as_vector(self):\n",
    "        \"\"\"\n",
    "        Concatenate weights and biases into a single vector.\n",
    "\n",
    "        Returns:\n",
    "        - Flattened vector containing weights and biases.\n",
    "        \"\"\"\n",
    "        return np.concatenate([\n",
    "            self.W1.flatten(), self.b1.flatten(),\n",
    "            self.W2.flatten(), self.b2.flatten(),\n",
    "            self.W3.flatten(), self.b3.flatten()\n",
    "        ])\n",
    "\n",
    "    def _set_weights_from_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Set weights and biases from a vector.\n",
    "\n",
    "        Parameters:\n",
    "        - vector: Vector containing weights and biases.\n",
    "        \"\"\"\n",
    "        sizes = [\n",
    "            self.W1.size, self.b1.size,\n",
    "            self.W2.size, self.b2.size,\n",
    "            self.W3.size, self.b3.size\n",
    "        ]\n",
    "        vectors = np.split(vector, np.cumsum(sizes)[:-1])\n",
    "        self.W1 = vectors[0].reshape(self.W1.shape)\n",
    "        self.b1 = vectors[1].reshape(self.b1.shape)\n",
    "        self.W2 = vectors[2].reshape(self.W2.shape)\n",
    "        self.b2 = vectors[3].reshape(self.b2.shape)\n",
    "        self.W3 = vectors[4].reshape(self.W3.shape)\n",
    "        self.b3 = vectors[5].reshape(self.b3.shape)\n",
    "\n",
    "    def _evaluate_fitness(self, weights_vector):\n",
    "        \"\"\"\n",
    "        Evaluate fitness using weights vector.\n",
    "\n",
    "        Parameters:\n",
    "        - weights_vector: Vector containing weights.\n",
    "\n",
    "        Returns:\n",
    "        - Negative accuracy for minimization.\n",
    "        \"\"\"\n",
    "        self._set_weights_from_vector(weights_vector)\n",
    "        self.forward_prop(X_train)\n",
    "        predictions = np.argmax(self.A3, axis=0)\n",
    "        return -np.sum(predictions == Y_train) / Y_train.size  # Negative accuracy for minimization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, method, epochs):\n",
    "        \"\"\"\n",
    "        Train the neural network using the specified method.\n",
    "\n",
    "        Parameters:\n",
    "        - method: String indicating the training method ('GD', 'SGD', 'adam', 'pso').\n",
    "        - epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        if method == 'GD':\n",
    "            self.train_GD(epochs)\n",
    "        elif method == 'SGD':\n",
    "            self.train_SGD(epochs)\n",
    "        elif method == 'adam':\n",
    "            self.train_adam(epochs)\n",
    "        elif method == 'pso':\n",
    "            self.train_pso(epochs)\n",
    "        else:\n",
    "            raise ValueError('Invalid Training Method')\n",
    "\n",
    "def ReLU(Z):\n",
    "    \"\"\"\n",
    "    ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the ReLU function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the ReLU function.\n",
    "    \"\"\"\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def ReLU_prime(Z):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the ReLU function.\n",
    "\n",
    "    Returns:\n",
    "    - Derivative of the ReLU function.\n",
    "    \"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the sigmoid function.\n",
    "    \"\"\"\n",
    "    A = 1 / (1 + np.exp(np.clip(-Z, -4, 4)))\n",
    "    return A\n",
    "\n",
    "def sigmoid_prime(Z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - Derivative of the sigmoid function.\n",
    "    \"\"\"\n",
    "    A = sigmoid(Z) * (1 - sigmoid(Z))\n",
    "    return A\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: Input to the softmax function.\n",
    "\n",
    "    Returns:\n",
    "    - Output of the softmax function.\n",
    "    \"\"\"\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # Shift values for numerical stability\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    return A\n",
    "\n",
    "def one_hot(Y):\n",
    "    \"\"\"\n",
    "    Convert class labels to one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "    - Y: Class labels.\n",
    "\n",
    "    Returns:\n",
    "    - One-hot encoded representation of class labels.\n",
    "    \"\"\"\n",
    "    one_hot_Y = np.zeros((Y.size, 10))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b208f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_Network(784, 32, 16, 10, 0.01, m_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3950c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 29.4244%\n",
      "Epoch 10: 66.0854%\n",
      "Epoch 20: 81.1098%\n",
      "Epoch 30: 84.7024%\n",
      "Epoch 40: 90.6537%\n",
      "Epoch 50: 92.6293%\n",
      "Epoch 60: 92.2537%\n",
      "Epoch 70: 93.3561%\n",
      "Epoch 80: 92.1439%\n",
      "Epoch 90: 94.478%\n",
      "Epoch 100: 95.1488%\n",
      "Epoch 110: 95.6268%\n",
      "Epoch 120: 95.2195%\n",
      "Epoch 130: 96.5024%\n",
      "Epoch 140: 94.7488%\n",
      "Epoch 150: 95.0805%\n",
      "Epoch 160: 93.3024%\n",
      "Epoch 170: 96.3146%\n",
      "Epoch 180: 96.1829%\n",
      "Epoch 190: 96.5366%\n",
      "Training complete\n",
      "\n",
      "Test Set Accuracy: 94.2%\n"
     ]
    }
   ],
   "source": [
    "nn.set_learning_rate(0.05)\n",
    "nn.train('adam', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d2bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
